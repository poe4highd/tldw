import os
import sys
import sqlite3
import yt_dlp
import whisper
import openai
import json
import re
from datetime import datetime

class Checkpoint:
    """æ£€æŸ¥ç‚¹å¸¸é‡å®šä¹‰"""
    DOWNLOAD = "download"
    TRANSCRIBE = "transcribe" 
    REPORT = "report"
    
class CheckpointStatus:
    """æ£€æŸ¥ç‚¹çŠ¶æ€å¸¸é‡"""
    PENDING = 0
    COMPLETED = 1

class LanguageConfig:
    """è¯­è¨€é…ç½®å’Œæ¨¡å‹æ˜ å°„"""
    # æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
    SUPPORTED_LANGUAGES = {
        'zh': 'ä¸­æ–‡',
        'en': 'English', 
        'ja': 'æ—¥æœ¬èª',
        'ko': 'í•œêµ­ì–´',
        'es': 'EspaÃ±ol',
        'fr': 'FranÃ§ais',
        'de': 'Deutsch',
        'it': 'Italiano',
        'pt': 'PortuguÃªs',
        'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹',
        'ar': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©',
        'hi': 'à¤¹à¤¿à¤¨à¥à¤¦à¥€'
    }
    
    # è¯­è¨€å¯¹åº”çš„æœ€ä½³Whisperæ¨¡å‹
    LANGUAGE_MODEL_MAP = {
        'zh': 'medium',     # ä¸­æ–‡ç”¨mediumæ¨¡å‹ï¼Œå¹³è¡¡æ•ˆæœå’Œé€Ÿåº¦
        'en': 'base',       # è‹±æ–‡ç”¨baseæ¨¡å‹ï¼Œæ•ˆæœå¥½ä¸”å¿«
        'ja': 'small',      # æ—¥æ–‡ç”¨smallæ¨¡å‹
        'ko': 'small',      # éŸ©æ–‡ç”¨smallæ¨¡å‹
        'es': 'base',       # è¥¿ç­ç‰™æ–‡ç”¨baseæ¨¡å‹
        'fr': 'base',       # æ³•æ–‡ç”¨baseæ¨¡å‹
        'de': 'base',       # å¾·æ–‡ç”¨baseæ¨¡å‹
        'default': 'small'  # é»˜è®¤ç”¨smallæ¨¡å‹
    }
    
    # è¯­è¨€æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
    LANGUAGE_DETECTION_THRESHOLD = 0.7
    
    @classmethod
    def get_optimal_model(cls, language):
        """æ ¹æ®è¯­è¨€è·å–æœ€ä½³æ¨¡å‹"""
        return cls.LANGUAGE_MODEL_MAP.get(language, cls.LANGUAGE_MODEL_MAP['default'])
    
    @classmethod 
    def get_language_name(cls, language_code):
        """è·å–è¯­è¨€çš„æ˜¾ç¤ºåç§°"""
        return cls.SUPPORTED_LANGUAGES.get(language_code, language_code)

class VideoProcessor:
    def __init__(self, database):
        self.db = database
        self.whisper_model = None
        self.openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.log_messages = []  # å­˜å‚¨è¯¦ç»†æ—¥å¿—æ¶ˆæ¯
        self.device = None  # ç¼“å­˜è®¾å¤‡ä¿¡æ¯
        
        # Whisperæ¨¡å‹ä¼˜å…ˆçº§ (æ•°å€¼è¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜)
        self.model_priority = {
            'tiny': 1,
            'base': 2,
            'small': 3,
            'medium': 4,
            'large': 5,
            'large-v2': 6,
            'large-v3': 7
        }
    
    def log(self, message):
        """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
        print(message)  # æœåŠ¡å™¨ç«¯æ—¥å¿—
        self.log_messages.append(message)  # æ”¶é›†ç”¨äºå‰ç«¯æ˜¾ç¤º
    
    def get_logs(self):
        """è·å–æ”¶é›†çš„æ—¥å¿—"""
        return '\n'.join(self.log_messages)
    
    def clear_logs(self):
        """æ¸…é™¤æ—¥å¿—"""
        self.log_messages = []
    
    def get_optimal_device(self):
        """è·å–æœ€ä¼˜è®¾å¤‡é…ç½®"""
        if self.device is None:
            import torch
            
            if torch.cuda.is_available():
                # æ£€æŸ¥GPUå†…å­˜
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
                self.device = {
                    'type': 'cuda',
                    'name': torch.cuda.get_device_name(0),
                    'memory': f"{gpu_memory:.1f}GB",
                    'optimal_model': 'medium' if gpu_memory > 4 else 'base'
                }
                self.log(f"ğŸ® æ£€æµ‹åˆ°GPU: {self.device['name']} ({self.device['memory']})")
            else:
                # CPUé…ç½®
                import psutil
                cpu_count = psutil.cpu_count()
                memory_gb = psutil.virtual_memory().total / 1024**3
                self.device = {
                    'type': 'cpu',
                    'name': f"{cpu_count}æ ¸CPU",
                    'memory': f"{memory_gb:.1f}GB",
                    'optimal_model': 'tiny' if memory_gb < 8 else 'base'
                }
                self.log(f"ğŸ’» ä½¿ç”¨CPU: {self.device['name']} ({self.device['memory']})")
        
        return self.device
    
    def extract_video_id(self, youtube_url):
        """ä»YouTube URLæå–è§†é¢‘ID"""
        # æ”¯æŒå¤šç§YouTube URLæ ¼å¼
        patterns = [
            r'(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/embed/)([^&\n?#]+)',
            r'youtube\.com/watch\?.*v=([^&\n?#]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, youtube_url)
            if match:
                video_id = match.group(1)
                # YouTubeè§†é¢‘IDé€šå¸¸æ˜¯11ä¸ªå­—ç¬¦
                if len(video_id) == 11:
                    return video_id
        
        # å¦‚æœæ— æ³•æå–ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise ValueError(f"æ— æ³•ä»URLæå–è§†é¢‘ID: {youtube_url}")
    
    def detect_audio_language(self, audio_file, video_id=None):
        """æ£€æµ‹éŸ³é¢‘æ–‡ä»¶çš„è¯­è¨€"""
        try:
            self.log("ğŸ” å¼€å§‹æ£€æµ‹éŸ³é¢‘è¯­è¨€...")
            
            # åŠ è½½ä¸€ä¸ªå°æ¨¡å‹ç”¨äºè¯­è¨€æ£€æµ‹
            detection_model = whisper.load_model("tiny")
            
            # åªåŠ è½½å‰30ç§’ç”¨äºè¯­è¨€æ£€æµ‹
            import whisper
            audio = whisper.load_audio(audio_file)
            audio = whisper.pad_or_trim(audio)  # å–å‰30ç§’
            
            # è·å–melé¢‘è°±
            mel = whisper.log_mel_spectrogram(audio).to(detection_model.device)
            
            # æ£€æµ‹è¯­è¨€
            _, probs = detection_model.detect_language(mel)
            detected_language = max(probs, key=probs.get)
            confidence = probs[detected_language]
            
            self.log(f"ğŸ” æ£€æµ‹åˆ°è¯­è¨€: {LanguageConfig.get_language_name(detected_language)} ({detected_language})")
            self.log(f"ğŸ“Š æ£€æµ‹ç½®ä¿¡åº¦: {confidence:.3f}")
            
            # å¦‚æœç½®ä¿¡åº¦è¶³å¤Ÿé«˜ï¼Œä¿å­˜æ£€æµ‹ç»“æœ
            if confidence >= LanguageConfig.LANGUAGE_DETECTION_THRESHOLD:
                if video_id:
                    self.db.update_language_info(video_id, detected_language=detected_language)
                self.log(f"âœ… è¯­è¨€æ£€æµ‹æˆåŠŸ: {LanguageConfig.get_language_name(detected_language)}")
                return detected_language, confidence
            else:
                self.log(f"âš ï¸ è¯­è¨€æ£€æµ‹ç½®ä¿¡åº¦ä¸è¶³ ({confidence:.3f} < {LanguageConfig.LANGUAGE_DETECTION_THRESHOLD})ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€")
                return 'zh', confidence  # é»˜è®¤ä¸­æ–‡
                
        except Exception as e:
            self.log(f"âŒ è¯­è¨€æ£€æµ‹å¤±è´¥: {str(e)}")
            return 'zh', 0.0  # é»˜è®¤ä¸­æ–‡
    
    def get_transcription_language(self, video_id):
        """è·å–è½¬å½•ä½¿ç”¨çš„è¯­è¨€"""
        # 1. ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·å¼ºåˆ¶æŒ‡å®šçš„è¯­è¨€
        lang_info = self.db.get_language_info(video_id)
        if lang_info and lang_info.get('forced_language'):
            self.log(f"ğŸ‘¤ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šè¯­è¨€: {LanguageConfig.get_language_name(lang_info['forced_language'])}")
            return lang_info['forced_language']
        
        # 2. ä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€
        if lang_info and lang_info.get('detected_language'):
            self.log(f"ğŸ” ä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€: {LanguageConfig.get_language_name(lang_info['detected_language'])}")
            return lang_info['detected_language']
        
        # 3. é»˜è®¤ä¸­æ–‡
        self.log("ğŸ“ ä½¿ç”¨é»˜è®¤è¯­è¨€: ä¸­æ–‡")
        return 'zh'

    def load_whisper_model(self, language=None):
        """å»¶è¿ŸåŠ è½½Whisperæ¨¡å‹ - æ™ºèƒ½é€‰æ‹©æ¨¡å‹å’Œè®¾å¤‡"""
        # æ ¹æ®è¯­è¨€ç¡®å®šæœ€ä½³æ¨¡å‹
        if language:
            optimal_model = LanguageConfig.get_optimal_model(language)
            self.log(f"ğŸ¯ æ ¹æ®è¯­è¨€ {LanguageConfig.get_language_name(language)} é€‰æ‹©æ¨¡å‹: {optimal_model}")
        else:
            # è·å–æœ€ä¼˜è®¾å¤‡é…ç½®çš„é»˜è®¤æ¨¡å‹
            device_info = self.get_optimal_device()
            optimal_model = device_info['optimal_model']
            
        if self.whisper_model is None or (language and optimal_model != getattr(self, 'current_model_name', None)):
            # è·å–æœ€ä¼˜è®¾å¤‡é…ç½®
            device_info = self.get_optimal_device()
            device = device_info['type']
            model_name = optimal_model
            
            self.log(f"ğŸ¤– Loading Whisper {model_name} model on {device}...")
            self.log(f"ğŸ“Š ç¡¬ä»¶é…ç½®: {device_info['name']} ({device_info['memory']})")
            
            try:
                # åŠ è½½æ¨¡å‹æ—¶æ·»åŠ æ›´å¤šé…ç½®
                if device == "cuda":
                    import torch
                    # æ¸…ç†GPUå†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                self.whisper_model = whisper.load_model(model_name, device=device)
                self.current_model_name = model_name  # è®°å½•å½“å‰æ¨¡å‹åç§°
                self.log(f"âœ… Whisper {model_name} æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {device})")
                
                # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
                model_params = sum(p.numel() for p in self.whisper_model.parameters()) / 1e6
                self.log(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {model_params:.1f}M")
                
            except Exception as e:
                # å¦‚æœé¦–é€‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æœ€å°æ¨¡å‹
                self.log(f"âš ï¸ {model_name}æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°tinyæ¨¡å‹: {str(e)}")
                try:
                    self.whisper_model = whisper.load_model("tiny", device="cpu")
                    self.current_model_name = "tiny"  # è®°å½•å›é€€æ¨¡å‹åç§°
                    self.log("âœ… Whisper tinyæ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: CPU)")
                except Exception as fallback_error:
                    raise Exception(f"Whisperæ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: {str(fallback_error)}")
                
        return self.whisper_model
    
    def should_reanalyze_with_better_model(self, video_id, current_model):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨æ›´å¥½çš„æ¨¡å‹é‡æ–°åˆ†æ"""
        # è·å–è¯¥è§†é¢‘ä¹‹å‰ä½¿ç”¨çš„æ¨¡å‹
        previous_model = self.db.get_video_whisper_model(video_id)
        
        if not previous_model:
            # é¦–æ¬¡åˆ†æï¼Œè®°å½•å½“å‰æ¨¡å‹
            self.db.update_whisper_model(video_id, current_model)
            return False, None
        
        # æ¯”è¾ƒæ¨¡å‹ä¼˜å…ˆçº§
        current_priority = self.model_priority.get(current_model, 0)
        previous_priority = self.model_priority.get(previous_model, 0)
        
        if current_priority > previous_priority:
            self.log(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹å‡çº§: {previous_model} â†’ {current_model}")
            self.log(f"ğŸ“ˆ æ¨¡å‹ä¼˜å…ˆçº§æå‡: {previous_priority} â†’ {current_priority}")
            return True, previous_model
        
        return False, previous_model
    
    # æ£€æŸ¥ç‚¹éªŒè¯å’Œç®¡ç†æ–¹æ³•
    def validate_checkpoint_status(self, video_id):
        """éªŒè¯æ‰€æœ‰æ£€æŸ¥ç‚¹çŠ¶æ€ï¼Œç¡®ä¿æ–‡ä»¶å­˜åœ¨æ€§"""
        self.log(f"ğŸ” éªŒè¯è§†é¢‘ {video_id} çš„æ£€æŸ¥ç‚¹çŠ¶æ€...")
        
        checkpoint_status = self.db.get_checkpoint_status(video_id)
        if not checkpoint_status:
            self.log(f"âŒ æ— æ³•è·å–è§†é¢‘ {video_id} çš„æ£€æŸ¥ç‚¹çŠ¶æ€")
            return None
        
        # éªŒè¯ä¸‹è½½æ£€æŸ¥ç‚¹
        download_valid = False
        if checkpoint_status['download_completed']:
            audio_path = checkpoint_status['audio_file_path']
            if audio_path and os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
                download_valid = True
                self.log(f"âœ… ä¸‹è½½æ£€æŸ¥ç‚¹éªŒè¯é€šè¿‡: {audio_path}")
            else:
                self.log(f"âŒ ä¸‹è½½æ£€æŸ¥ç‚¹å¤±æ•ˆ: æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º")
                self.db.reset_checkpoint(video_id, Checkpoint.DOWNLOAD)
        
        # éªŒè¯è½¬å½•æ£€æŸ¥ç‚¹
        transcribe_valid = False
        if checkpoint_status['transcribe_completed']:
            transcript_path = checkpoint_status['transcript_file_path']
            if transcript_path:
                # æ£€æŸ¥SRTå’ŒTXTæ–‡ä»¶
                srt_file = transcript_path if transcript_path.endswith('.srt') else transcript_path + '.srt'
                txt_file = transcript_path.replace('.srt', '.txt') if transcript_path.endswith('.srt') else transcript_path + '.txt'
                
                if (os.path.exists(srt_file) and os.path.getsize(srt_file) > 0 and 
                    os.path.exists(txt_file) and os.path.getsize(txt_file) > 0):
                    transcribe_valid = True
                    self.log(f"âœ… è½¬å½•æ£€æŸ¥ç‚¹éªŒè¯é€šè¿‡: {srt_file}, {txt_file}")
                else:
                    self.log(f"âŒ è½¬å½•æ£€æŸ¥ç‚¹å¤±æ•ˆ: è½¬å½•æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º")
                    self.db.reset_checkpoint(video_id, Checkpoint.TRANSCRIBE)
        
        # éªŒè¯ç®€æŠ¥æ£€æŸ¥ç‚¹
        report_valid = False
        if checkpoint_status['report_completed']:
            report_filename = checkpoint_status['report_filename']
            if report_filename:
                report_path = f"reports/{report_filename}"
                if os.path.exists(report_path) and os.path.getsize(report_path) > 0:
                    report_valid = True
                    self.log(f"âœ… ç®€æŠ¥æ£€æŸ¥ç‚¹éªŒè¯é€šè¿‡: {report_path}")
                else:
                    self.log(f"âŒ ç®€æŠ¥æ£€æŸ¥ç‚¹å¤±æ•ˆ: ç®€æŠ¥æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º")
                    self.db.reset_checkpoint(video_id, Checkpoint.REPORT)
        
        return {
            'download_valid': download_valid,
            'transcribe_valid': transcribe_valid,
            'report_valid': report_valid,
            'checkpoint_status': checkpoint_status
        }
    
    def get_next_checkpoint(self, video_id):
        """ç¡®å®šä¸‹ä¸€ä¸ªéœ€è¦æ‰§è¡Œçš„æ£€æŸ¥ç‚¹"""
        validation = self.validate_checkpoint_status(video_id)
        if not validation:
            return Checkpoint.DOWNLOAD
        
        if not validation['download_valid']:
            self.log(f"ğŸ“ ä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹: {Checkpoint.DOWNLOAD}")
            return Checkpoint.DOWNLOAD
        elif not validation['transcribe_valid']:
            self.log(f"ğŸ“ ä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹: {Checkpoint.TRANSCRIBE}")
            return Checkpoint.TRANSCRIBE
        elif not validation['report_valid']:
            self.log(f"ğŸ“ ä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹: {Checkpoint.REPORT}")
            return Checkpoint.REPORT
        else:
            self.log(f"âœ… æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½å·²å®Œæˆ")
            return None
    
    def is_fully_completed(self, video_id):
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦å®Œå…¨å¤„ç†å®Œæˆ"""
        validation = self.validate_checkpoint_status(video_id)
        if not validation:
            return False
        
        return (validation['download_valid'] and 
                validation['transcribe_valid'] and 
                validation['report_valid'])
    
    def sync_checkpoints_with_files(self, video_id):
        """åŒæ­¥æ–‡ä»¶çŠ¶æ€åˆ°æ•°æ®åº“æ£€æŸ¥ç‚¹"""
        self.log(f"ğŸ”„ åŒæ­¥è§†é¢‘ {video_id} çš„æ–‡ä»¶çŠ¶æ€åˆ°æ£€æŸ¥ç‚¹...")
        
        # è¿™å°†é€šè¿‡validate_checkpoint_statusè‡ªåŠ¨é‡ç½®å¤±æ•ˆçš„æ£€æŸ¥ç‚¹
        self.validate_checkpoint_status(video_id)
        self.log(f"âœ… æ£€æŸ¥ç‚¹åŒæ­¥å®Œæˆ")
    
    def get_current_optimal_model(self):
        """è·å–å½“å‰ç¯å¢ƒä¸‹çš„æœ€ä¼˜æ¨¡å‹"""
        device_info = self.get_optimal_device()
        return device_info['optimal_model']
    
    def download_audio_fallback(self, youtube_url, video_id):
        """å¤‡ç”¨ä¸‹è½½æ–¹æ³• - ä½¿ç”¨æœ€ç®€é…ç½®"""
        strategies = [
            # ç­–ç•¥1: ä½¿ç”¨Androidå®¢æˆ·ç«¯
            {
                'format': 'bestaudio/best',
                'outtmpl': f'downloads/%(title)s.%(ext)s',
                'extractor_args': {'youtube': {'player_client': ['android']}},
                'user_agent': 'com.google.android.youtube/17.31.35 (Linux; U; Android 11) gzip',
            },
            # ç­–ç•¥2: ä½¿ç”¨iOSå®¢æˆ·ç«¯
            {
                'format': 'bestaudio/best', 
                'outtmpl': f'downloads/%(title)s.%(ext)s',
                'extractor_args': {'youtube': {'player_client': ['ios']}},
                'user_agent': 'com.google.ios.youtube/17.31.4 (iPhone; CPU iPhone OS 15_6 like Mac OS X)',
            },
            # ç­–ç•¥3: æœ€åŸºæœ¬é…ç½®
            {
                'format': 'worst[ext=webm]/worst',
                'outtmpl': f'downloads/%(title)s.%(ext)s',
                'no_warnings': True,
                'quiet': True,
            }
        ]
        
        for i, ydl_opts in enumerate(strategies, 1):
            try:
                self.log(f"ğŸ“± å°è¯•å¤‡ç”¨ç­–ç•¥ {i}...")
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(youtube_url, download=False)
                    video_title = info.get('title', 'Unknown Title')
                    
                    # æ›´æ–°æ•°æ®åº“ä¸­çš„è§†é¢‘æ ‡é¢˜
                    with sqlite3.connect(self.db.db_path) as conn:
                        cursor = conn.cursor()
                        cursor.execute('UPDATE videos SET video_title=? WHERE id=?', (video_title, video_id))
                        conn.commit()
                    
                    # ä¸‹è½½éŸ³é¢‘
                    ydl.download([youtube_url])
                    
                    # æ‰¾åˆ°ä¸‹è½½çš„æ–‡ä»¶
                    safe_title = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
                    # æ£€æŸ¥å¯èƒ½çš„æ–‡ä»¶æ ¼å¼
                    for ext in ['.webm', '.mp4', '.m4a', '.mp3']:
                        audio_file = f"downloads/{safe_title}{ext}"
                        if os.path.exists(audio_file):
                            return audio_file, video_title
                    
                    raise Exception("æ‰¾ä¸åˆ°ä¸‹è½½çš„éŸ³é¢‘æ–‡ä»¶")
                    
            except Exception as e:
                self.log(f"âŒ å¤‡ç”¨ç­–ç•¥ {i} å¤±è´¥: {str(e)}")
                continue
        
        raise Exception("æ‰€æœ‰å¤‡ç”¨ç­–ç•¥éƒ½å¤±è´¥äº†")

    def download_audio_final_fallback(self, youtube_url, video_id):
        """æœ€ç»ˆå¤‡ç”¨æ–¹æ¡ˆ - å¤åˆ¶æµ‹è¯•è„šæœ¬çš„ç¡®åˆ‡é…ç½®"""
        try:
            self.log("ğŸ¯ ä½¿ç”¨æµ‹è¯•è„šæœ¬éªŒè¯çš„ç¡®åˆ‡é…ç½®...")
            
            # å®Œå…¨å¤åˆ¶æµ‹è¯•è„šæœ¬ä¸­æˆåŠŸçš„é…ç½®
            ydl_opts = {
                'format': 'bestaudio/best',
                'outtmpl': f'downloads/final_%(title)s.%(ext)s',
                'postprocessors': [{
                    'key': 'FFmpegExtractAudio',
                    'preferredcodec': 'mp3',
                    'preferredquality': '192',
                }],
                'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'referer': 'https://www.youtube.com/',
                'extractor_args': {
                    'youtube': {
                        'skip': ['dash'],
                        'player_skip': ['js'],
                        'player_client': ['web', 'android'],
                    }
                },
                'cookiesfrombrowser': ('firefox', None, None, None),
                'http_headers': {
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Encoding': 'gzip, deflate',
                    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
                    'Connection': 'keep-alive',
                },
                'no_warnings': True,
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                self.log("ğŸ“‹ è·å–è§†é¢‘ä¿¡æ¯...")
                info = ydl.extract_info(youtube_url, download=False)
                video_title = info.get('title', 'Unknown Title')
                
                self.log(f"âœ… è§†é¢‘æ ‡é¢˜: {video_title}")
                
                # æ›´æ–°æ•°æ®åº“ä¸­çš„è§†é¢‘æ ‡é¢˜
                with sqlite3.connect(self.db.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('UPDATE videos SET video_title=? WHERE id=?', (video_title, video_id))
                    conn.commit()
                
                self.log("â¬‡ï¸ å¼€å§‹ä¸‹è½½...")
                ydl.download([youtube_url])
                
                # æ‰¾åˆ°ä¸‹è½½çš„æ–‡ä»¶
                safe_title = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
                audio_file = f"downloads/final_{safe_title}.mp3"
                
                if os.path.exists(audio_file):
                    self.log(f"ğŸ‰ ä¸‹è½½æˆåŠŸ: {audio_file}")
                    return audio_file, video_title
                else:
                    # å°è¯•å¯»æ‰¾å…¶ä»–å¯èƒ½çš„æ–‡ä»¶å
                    for prefix in ['final_', '']:
                        for ext in ['.mp3', '.m4a', '.webm', '.mp4']:
                            test_file = f"downloads/{prefix}{safe_title}{ext}"
                            if os.path.exists(test_file):
                                return test_file, video_title
                    
                    raise Exception("æ‰¾ä¸åˆ°ä¸‹è½½çš„æ–‡ä»¶")
                
        except Exception as e:
            raise Exception(f"æœ€ç»ˆå¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {str(e)}")

    def download_audio_ultra_simple(self, youtube_url, video_id):
        """ç»ˆæç®€åŒ–æ–¹æ¡ˆ - æœ€åŸºæœ¬çš„é…ç½®"""
        try:
            print("ä½¿ç”¨ç»ˆæç®€åŒ–é…ç½®...")
            
            # æœ€ç®€å•çš„é…ç½®ï¼Œåªä¸‹è½½ä¸è½¬æ¢
            ydl_opts = {
                'outtmpl': f'downloads/ultra_%(title)s.%(ext)s',
                'format': 'worst',
                'quiet': False,
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                print("è·å–è§†é¢‘ä¿¡æ¯...")
                info = ydl.extract_info(youtube_url, download=False)
                video_title = info.get('title', 'Unknown Title')
                
                print(f"è§†é¢‘æ ‡é¢˜: {video_title}")
                
                # æ›´æ–°æ•°æ®åº“
                with sqlite3.connect(self.db.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('UPDATE videos SET video_title=? WHERE id=?', (video_title, video_id))
                    conn.commit()
                
                print("å¼€å§‹ä¸‹è½½ (ä¸è½¬æ¢æ ¼å¼)...")
                ydl.download([youtube_url])
                
                # æŸ¥æ‰¾ä¸‹è½½çš„æ–‡ä»¶
                safe_title = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
                
                # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶
                import glob
                pattern = f"downloads/ultra_{safe_title}.*"
                files = glob.glob(pattern)
                
                if files:
                    audio_file = files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
                    print(f"æ‰¾åˆ°æ–‡ä»¶: {audio_file}")
                    return audio_file, video_title
                else:
                    # åˆ—å‡ºdownloadsç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
                    import os
                    if os.path.exists('downloads'):
                        all_files = os.listdir('downloads')
                        ultra_files = [f for f in all_files if f.startswith('ultra_')]
                        if ultra_files:
                            audio_file = f"downloads/{ultra_files[0]}"
                            return audio_file, video_title
                    
                    raise Exception("æ‰¾ä¸åˆ°ä¸‹è½½çš„æ–‡ä»¶")
                
        except Exception as e:
            raise Exception(f"ç»ˆæç®€åŒ–æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(e)}")

    def download_audio(self, youtube_url, video_id):
        """ä¸‹è½½YouTubeéŸ³é¢‘ - ä½¿ç”¨è§†é¢‘IDä½œä¸ºæ–‡ä»¶å"""
        try:
            self.clear_logs()  # æ¸…é™¤ä¹‹å‰çš„æ—¥å¿—
            
            # æå–YouTubeè§†é¢‘ID
            try:
                yt_video_id = self.extract_video_id(youtube_url)
                self.log(f"âœ… æå–è§†é¢‘ID: {yt_video_id}")
            except ValueError as e:
                self.log(f"âŒ {str(e)}")
                raise
            
            # æ£€æŸ¥MP3æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            expected_mp3 = f"downloads/{yt_video_id}.mp3"
            if os.path.exists(expected_mp3):
                file_size = os.path.getsize(expected_mp3) / (1024 * 1024)  # MB
                self.log(f"ğŸ‰ å‘ç°å·²å­˜åœ¨çš„MP3æ–‡ä»¶: {expected_mp3} ({file_size:.2f} MB)")
                self.log("â­ï¸ è·³è¿‡ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶")
                
                # ä»æ•°æ®åº“è·å–è§†é¢‘æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•è·å–
                with sqlite3.connect(self.db.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('SELECT video_title FROM videos WHERE id=?', (video_id,))
                    result = cursor.fetchone()
                    video_title = result[0] if result and result[0] else None
                
                # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ ‡é¢˜ï¼Œåˆ™è·å–è§†é¢‘ä¿¡æ¯
                if not video_title:
                    self.log("ğŸ“‹ è·å–è§†é¢‘æ ‡é¢˜ä¿¡æ¯...")
                    ydl_opts = {'quiet': True}
                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                        info = ydl.extract_info(youtube_url, download=False)
                        video_title = info.get('title', 'Unknown Title')
                        # æ›´æ–°æ•°æ®åº“ä¸­çš„è§†é¢‘æ ‡é¢˜
                        with sqlite3.connect(self.db.db_path) as conn:
                            cursor = conn.cursor()
                            cursor.execute('UPDATE videos SET video_title=? WHERE id=?', (video_title, video_id))
                            conn.commit()
                        self.log(f"âœ… è§†é¢‘æ ‡é¢˜: {video_title}")
                
                return expected_mp3, video_title
            
            self.log("="*60)
            self.log("ğŸ¯ å¼€å§‹YouTubeä¸‹è½½è¿‡ç¨‹")
            self.log(f"ğŸ“¹ URL: {youtube_url}")
            self.log(f"ğŸ†” æ•°æ®åº“ID: {video_id}")
            self.log(f"ğŸ¬ YouTubeè§†é¢‘ID: {yt_video_id}")
            self.log("ğŸ”§ ç­–ç•¥: ä½¿ç”¨è§†é¢‘IDä½œä¸ºæ–‡ä»¶å")
            self.log("="*60)
            
            # ä½¿ç”¨è§†é¢‘IDä½œä¸ºæ–‡ä»¶åçš„é…ç½®
            ydl_opts = {
                'format': 'bestaudio/best',
                'outtmpl': f'downloads/{yt_video_id}.%(ext)s',
                'postprocessors': [{
                    'key': 'FFmpegExtractAudio',
                    'preferredcodec': 'mp3',
                    'preferredquality': '192',
                }],
                'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'referer': 'https://www.youtube.com/',
                'extractor_args': {
                    'youtube': {
                        'skip': ['dash'],
                        'player_skip': ['js'],
                        'player_client': ['web', 'android'],
                    }
                },
                'http_headers': {
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Encoding': 'gzip, deflate',
                    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
                    'Connection': 'keep-alive',
                },
                'no_warnings': True,
            }
            
            # æ·»åŠ è¯¦ç»†çš„ç¯å¢ƒå’Œé…ç½®æ—¥å¿—
            self.log(f"ğŸ“± Flaskè¿›ç¨‹ç¯å¢ƒä¿¡æ¯:")
            self.log(f"   ğŸ Pythonæ‰§è¡Œè·¯å¾„: {sys.executable}")
            self.log(f"   ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            self.log(f"   ğŸ“¦ yt-dlpç‰ˆæœ¬: {yt_dlp.version.__version__}")
            
            self.log(f"ğŸ”§ yt-dlpé…ç½®:")
            self.log(f"   ğŸµ æ ¼å¼: {ydl_opts['format']}")
            self.log(f"   ğŸ•·ï¸ User-Agent: {ydl_opts['user_agent'][:50]}...")
            self.log(f"   ğŸ”— Referer: {ydl_opts.get('referer', 'æœªè®¾ç½®')}")
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                self.log("ğŸ“‹ å¼€å§‹è·å–è§†é¢‘ä¿¡æ¯...")
                info = ydl.extract_info(youtube_url, download=False)
                video_title = info.get('title', 'Unknown Title')
                
                self.log(f"âœ… è§†é¢‘æ ‡é¢˜: {video_title}")
                self.log(f"âœ… è§†é¢‘æ—¶é•¿: {info.get('duration', 'Unknown')}ç§’")
                self.log(f"âœ… ä¸Šä¼ è€…: {info.get('uploader', 'Unknown')}")
                
                # æ›´æ–°æ•°æ®åº“ä¸­çš„è§†é¢‘æ ‡é¢˜
                with sqlite3.connect(self.db.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('UPDATE videos SET video_title=? WHERE id=?', (video_title, video_id))
                    conn.commit()
                
                self.log("â¬‡ï¸ å¼€å§‹ä¸‹è½½...")
                ydl.download([youtube_url])
                
                # ä½¿ç”¨è§†é¢‘IDæŸ¥æ‰¾ä¸‹è½½çš„æ–‡ä»¶
                expected_mp3 = f"downloads/{yt_video_id}.mp3"
                
                # é¦–å…ˆæ£€æŸ¥MP3æ–‡ä»¶ï¼ˆè½¬æ¢åçš„ç›®æ ‡æ ¼å¼ï¼‰
                if os.path.exists(expected_mp3):
                    file_size = os.path.getsize(expected_mp3) / (1024 * 1024)  # MB
                    self.log(f"ğŸ‰ ä¸‹è½½æˆåŠŸ: {expected_mp3} ({file_size:.2f} MB)")
                    return expected_mp3, video_title
                
                # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„æ ¼å¼ï¼ˆæœªè½¬æ¢çš„åŸå§‹æ ¼å¼ï¼‰
                for ext in ['.m4a', '.webm', '.mp4']:
                    test_file = f"downloads/{yt_video_id}{ext}"
                    if os.path.exists(test_file):
                        file_size = os.path.getsize(test_file) / (1024 * 1024)  # MB
                        self.log(f"ğŸ‰ ä¸‹è½½æˆåŠŸ (æ ¼å¼: {ext}): {test_file} ({file_size:.2f} MB)")
                        return test_file, video_title
                
                # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ—å‡ºdownloadsç›®å½•å†…å®¹è¿›è¡Œè°ƒè¯•
                self.log("ğŸ” downloadsç›®å½•å†…å®¹:")
                try:
                    for f in os.listdir("downloads"):
                        if f.startswith(yt_video_id):
                            self.log(f"   ğŸ“„ æ‰¾åˆ°ç›¸å…³æ–‡ä»¶: {f}")
                except Exception as e:
                    self.log(f"   âŒ æ— æ³•åˆ—å‡ºç›®å½•: {e}")
                
                raise Exception(f"æ‰¾ä¸åˆ°è§†é¢‘IDä¸º {yt_video_id} çš„ä¸‹è½½æ–‡ä»¶")
                
        except Exception as e:
            self.log("âŒ Androidå®¢æˆ·ç«¯ç­–ç•¥å¤±è´¥!")
            self.log(f"ğŸ” é”™è¯¯è¯¦æƒ…: {str(e)}")
            self.log("\n" + "="*60)
            self.log("ğŸ”„ å°è¯•iOSå®¢æˆ·ç«¯å¤‡ç”¨ç­–ç•¥")
            self.log("="*60)
            
            try:
                # å°è¯•iOSå®¢æˆ·ç«¯
                self.log("ğŸ“± ä½¿ç”¨iOSå®¢æˆ·ç«¯é…ç½®...")
                ios_opts = {
                    'format': 'bestaudio/best',
                    'outtmpl': f'downloads/%(title)s.%(ext)s',
                    'postprocessors': [{
                        'key': 'FFmpegExtractAudio',
                        'preferredcodec': 'mp3',
                        'preferredquality': '192',
                    }],
                    'extractor_args': {'youtube': {'player_client': ['ios']}},
                    'user_agent': 'com.google.ios.youtube/17.31.4 (iPhone; CPU iPhone OS 15_6 like Mac OS X)',
                    'no_warnings': True,
                }
                
                with yt_dlp.YoutubeDL(ios_opts) as ydl:
                    info = ydl.extract_info(youtube_url, download=False)
                    video_title = info.get('title', 'Unknown Title')
                    self.log(f"âœ… iOSç­–ç•¥è·å–æ ‡é¢˜: {video_title}")
                    
                    # æ›´æ–°æ•°æ®åº“
                    with sqlite3.connect(self.db.db_path) as conn:
                        cursor = conn.cursor()
                        cursor.execute('UPDATE videos SET video_title=? WHERE id=?', (video_title, video_id))
                        conn.commit()
                    
                    ydl.download([youtube_url])
                    
                    # æŸ¥æ‰¾æ–‡ä»¶
                    safe_title = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
                    for ext in ['.mp3', '.m4a', '.webm', '.mp4']:
                        audio_file = f"downloads/{safe_title}{ext}"
                        if os.path.exists(audio_file):
                            self.log(f"ğŸ‰ iOSç­–ç•¥æˆåŠŸ: {audio_file}")
                            return audio_file, video_title
                    
                    raise Exception("iOSç­–ç•¥ä¸‹è½½å®Œæˆä½†æ‰¾ä¸åˆ°æ–‡ä»¶")
                    
            except Exception as ios_error:
                self.log("âŒ iOSç­–ç•¥ä¹Ÿå¤±è´¥!")
                self.log(f"ğŸ” é”™è¯¯è¯¦æƒ…: {str(ios_error)}")
                
                # æœ€ç®€åŒ–ç­–ç•¥ - åªä¸‹è½½ä¸è½¬æ¢
                self.log("\nğŸš€ å°è¯•æœ€ç®€åŒ–ç­–ç•¥ (ä¸è½¬æ¢æ ¼å¼)...")
                try:
                    simple_opts = {
                        'format': 'worst[ext=webm]/worst',
                        'outtmpl': f'downloads/%(title)s.%(ext)s',
                        'no_warnings': True,
                    }
                    
                    with yt_dlp.YoutubeDL(simple_opts) as ydl:
                        info = ydl.extract_info(youtube_url, download=False)
                        video_title = info.get('title', 'Unknown Title')
                        self.log(f"âœ… æœ€ç®€ç­–ç•¥è·å–æ ‡é¢˜: {video_title}")
                        
                        # æ›´æ–°æ•°æ®åº“
                        with sqlite3.connect(self.db.db_path) as conn:
                            cursor = conn.cursor()
                            cursor.execute('UPDATE videos SET video_title=? WHERE id=?', (video_title, video_id))
                            conn.commit()
                        
                        ydl.download([youtube_url])
                        
                        # æŸ¥æ‰¾ä»»æ„æ ¼å¼çš„æ–‡ä»¶
                        safe_title = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
                        for ext in ['.webm', '.mp4', '.m4a', '.mp3']:
                            audio_file = f"downloads/{safe_title}{ext}"
                            if os.path.exists(audio_file):
                                self.log(f"ğŸ‰ æœ€ç®€ç­–ç•¥æˆåŠŸ: {audio_file}")
                                return audio_file, video_title
                        
                        raise Exception("æœ€ç®€ç­–ç•¥ä¸‹è½½å®Œæˆä½†æ‰¾ä¸åˆ°æ–‡ä»¶")
                        
                except Exception as simple_error:
                    self.log("âŒ æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥äº†!")
                    
                    # è·å–å®Œæ•´çš„æ—¥å¿—ä¿¡æ¯
                    detailed_logs = self.get_logs()
                    error_summary = f"""æ‰€æœ‰ä¸‹è½½ç­–ç•¥éƒ½å¤±è´¥äº†ï¼

è¯¦ç»†æ—¥å¿—:
{detailed_logs}

é”™è¯¯æ±‡æ€»:
1ï¸âƒ£ Androidç­–ç•¥: {str(e)}
2ï¸âƒ£ iOSç­–ç•¥: {str(ios_error)}
3ï¸âƒ£ æœ€ç®€ç­–ç•¥: {str(simple_error)}"""
                    raise Exception(error_summary)
    
    def transcribe_audio(self, audio_file, video_id=None, force_retranscribe=False):
        """ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘ - æ”¯æŒæ™ºèƒ½è¯­è¨€æ£€æµ‹"""
        try:
            # æ£€æŸ¥è½¬å½•æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            base_name = os.path.splitext(os.path.basename(audio_file))[0]
            srt_file = f"transcripts/{base_name}.srt"
            transcript_file = f"transcripts/{base_name}.txt"
            
            if not force_retranscribe and os.path.exists(srt_file) and os.path.exists(transcript_file):
                self.log(f"ğŸ‰ å‘ç°å·²å­˜åœ¨çš„è½¬å½•æ–‡ä»¶: {srt_file}")
                self.log("â­ï¸ è·³è¿‡è½¬å½•ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶")
                
                # è¯»å–ç°æœ‰çš„è½¬å½•æ–‡æœ¬
                with open(transcript_file, 'r', encoding='utf-8') as f:
                    transcript_text = f.read()
                
                # è§£æSRTæ–‡ä»¶è·å–segmentsä¿¡æ¯ï¼Œå¹¶åˆå¹¶çŸ­ç‰‡æ®µ
                raw_segments = self.parse_srt_file(srt_file)
                merged_segments = self.merge_short_segments(raw_segments)
                
                self.log(f"ğŸ“Š åŸå§‹ç‰‡æ®µæ•°: {len(raw_segments)}, åˆå¹¶åç‰‡æ®µæ•°: {len(merged_segments)}")
                
                return transcript_text, srt_file, merged_segments
            
            if force_retranscribe:
                self.log(f"ğŸ”„ å¼ºåˆ¶é‡æ–°è½¬å½• (ä½¿ç”¨æ›´å¥½çš„æ¨¡å‹)")
            elif os.path.exists(srt_file) or os.path.exists(transcript_file):
                self.log(f"ğŸ”„ è¦†ç›–ç°æœ‰è½¬å½•æ–‡ä»¶ (æ¨¡å‹å‡çº§)")
            
            # æ™ºèƒ½è¯­è¨€æ£€æµ‹å’Œæ¨¡å‹é€‰æ‹©
            if video_id:
                # é¦–å…ˆå°è¯•æ£€æµ‹è¯­è¨€(å¦‚æœè¿˜æ²¡æœ‰æ£€æµ‹è¿‡)
                lang_info = self.db.get_language_info(video_id)
                if not lang_info or not lang_info.get('detected_language'):
                    detected_lang, confidence = self.detect_audio_language(audio_file, video_id)
                
                # è·å–è½¬å½•è¯­è¨€
                transcription_language = self.get_transcription_language(video_id)
            else:
                # æ²¡æœ‰video_idæ—¶ï¼Œç›´æ¥æ£€æµ‹è¯­è¨€
                transcription_language, _ = self.detect_audio_language(audio_file)
            
            # æ ¹æ®è¯­è¨€åŠ è½½æœ€ä½³æ¨¡å‹
            model = self.load_whisper_model(transcription_language)
            self.log(f"ğŸ™ï¸ å¼€å§‹è½¬å½•éŸ³é¢‘æ–‡ä»¶: {audio_file}")
            self.log(f"ğŸŒ ä½¿ç”¨è¯­è¨€: {LanguageConfig.get_language_name(transcription_language)} ({transcription_language})")
            
            # ä¼˜åŒ–çš„è½¬å½•å‚æ•° - ä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€
            transcribe_options = {
                'language': transcription_language,  # ä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€
                'fp16': False,     # CPUä¸‹å…³é—­fp16
                'task': 'transcribe',  # æ˜ç¡®æŒ‡å®šä»»åŠ¡ç±»å‹
                'verbose': False,  # å‡å°‘å†—ä½™è¾“å‡º
                'word_timestamps': True,  # å¯ç”¨è¯çº§æ—¶é—´æˆ³ï¼Œæœ‰åŠ©äºæ›´å¥½çš„åˆ†æ®µ
                'condition_on_previous_text': True,  # åŸºäºå‰æ–‡ä¸Šä¸‹æ–‡ï¼Œæé«˜è¿è´¯æ€§
            }
            
            # å¦‚æœæ˜¯GPUï¼Œå¯ç”¨ä¸€äº›ä¼˜åŒ–é€‰é¡¹
            import torch
            if torch.cuda.is_available():
                transcribe_options['fp16'] = True  # GPUä¸‹å¯ç”¨fp16åŠ é€Ÿ
                print("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿè½¬å½•...")
            else:
                print("ğŸ’» ä½¿ç”¨CPUè½¬å½•...")
            
            result = model.transcribe(audio_file, **transcribe_options)
            original_segments = result.get('segments', [])
            print(f"âœ… è½¬å½•å®Œæˆï¼Œè¯†åˆ«åˆ° {len(original_segments)} ä¸ªåŸå§‹è¯­éŸ³ç‰‡æ®µ")
            
            # åˆå¹¶çŸ­ç‰‡æ®µä»¥å‡å°‘ç‰‡æ®µæ•°é‡
            merged_segments = self.merge_short_segments(original_segments)
            print(f"ğŸ“Š åˆå¹¶çŸ­ç‰‡æ®µå: {len(merged_segments)} ä¸ªç‰‡æ®µ")
            
            # ç”ŸæˆSRTæ ¼å¼å­—å¹•ï¼ˆä½¿ç”¨åˆå¹¶åçš„ç‰‡æ®µï¼‰
            srt_content = self.generate_srt(merged_segments)
            
            # ç¡®ä¿transcriptsç›®å½•å­˜åœ¨
            os.makedirs('transcripts', exist_ok=True)
            
            # ä¿å­˜SRTæ–‡ä»¶
            with open(srt_file, 'w', encoding='utf-8') as f:
                f.write(srt_content)
            
            # GPTå­—å¹•æ ¡æ­£
            self.log("ğŸ” å¼€å§‹GPTå­—å¹•æ ¡æ­£...")
            corrected_text = self.correct_transcript_with_gpt(result['text'], transcription_language)
            
            # ä¿å­˜æ ¡æ­£åçš„çº¯æ–‡æœ¬è½¬å½•
            with open(transcript_file, 'w', encoding='utf-8') as f:
                f.write(corrected_text)
            
            # è®¡ç®—å¹¶ä¿å­˜å­—å¹•è´¨é‡è¯„åˆ†
            if video_id:
                quality_score = self._calculate_text_quality_score(corrected_text)
                self.db.update_subtitle_quality(video_id, quality_score)
            
            print(f"âœ… è½¬å½•å®Œæˆï¼Œä¿å­˜åˆ°: {srt_file}")
            
            return corrected_text, srt_file, merged_segments
            
        except Exception as e:
            raise Exception(f"è¯­éŸ³è½¬å½•å¤±è´¥: {str(e)}")
    
    def is_sentence_end(self, text):
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºå¥å­ç»“å°¾"""
        # ä¸­æ–‡å¥å­ç»“å°¾æ ‡ç‚¹
        chinese_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›']
        # è‹±æ–‡å¥å­ç»“å°¾æ ‡ç‚¹  
        english_endings = ['.', '!', '?', ';']
        
        text = text.strip()
        if not text:
            return False
            
        last_char = text[-1]
        return last_char in chinese_endings or last_char in english_endings
    
    def is_natural_pause(self, text):
        """åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªç„¶åœé¡¿ç‚¹ï¼ˆé€—å·ã€å†’å·ç­‰ï¼‰"""
        pause_marks = ['ï¼Œ', 'ã€', 'ï¼š', 'ï¼›', ',', ':', ';', '--', 'â€”â€”']
        text = text.strip()
        if not text:
            return False
        return text[-1] in pause_marks
    
    def calculate_sentence_score(self, text):
        """è®¡ç®—æ–‡æœ¬çš„å¥å­å®Œæ•´æ€§è¯„åˆ†"""
        score = 0
        text = text.strip()
        
        # å¥å­ç»“å°¾æ ‡ç‚¹åŠ åˆ†
        if self.is_sentence_end(text):
            score += 10
        
        # è‡ªç„¶åœé¡¿ç‚¹åŠ åˆ†
        elif self.is_natural_pause(text):
            score += 5
        
        # é•¿åº¦è¯„åˆ†ï¼ˆ20-80å­—ç¬¦è¾ƒç†æƒ³ï¼‰
        length = len(text)
        if 20 <= length <= 80:
            score += 8
        elif 10 <= length <= 120:
            score += 5
        elif length < 10:
            score -= 3
        
        # å®Œæ•´è¯æ±‡ç»“å°¾åŠ åˆ†
        if text and text[-1].isalnum():
            score += 2
        
        return score

    def merge_short_segments(self, segments, target_duration=25.0, max_duration=45.0):
        """
        æ™ºèƒ½åˆå¹¶çŸ­ç‰‡æ®µï¼Œä¼˜åŒ–å¥å­å®Œæ•´æ€§å’Œè‡ªç„¶åº¦
        
        Args:
            segments: åŸå§‹ç‰‡æ®µåˆ—è¡¨
            target_duration: ç›®æ ‡ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰
            max_duration: æœ€å¤§ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰
        """
        if not segments:
            return segments
        
        self.log("ğŸ“ å¼€å§‹æ™ºèƒ½å­—å¹•åˆå¹¶...")
        
        merged_segments = []
        current_segment = None
        
        for i, segment in enumerate(segments):
            if not isinstance(segment, dict):
                continue
                
            start = segment.get('start', 0)
            end = segment.get('end', 0)
            text = segment.get('text', '').strip()
            
            if not text:
                continue
            
            if current_segment is None:
                # å¼€å§‹æ–°ç‰‡æ®µ
                current_segment = {
                    'start': start,
                    'end': end,
                    'text': text,
                    'original_segments': [segment]
                }
                continue
            
            # è®¡ç®—å½“å‰ç‰‡æ®µä¿¡æ¯
            current_duration = current_segment['end'] - current_segment['start']
            gap = start - current_segment['end']
            new_duration = end - current_segment['start']
            
            # å¥å­å®Œæ•´æ€§è¯„åˆ†
            current_score = self.calculate_sentence_score(current_segment['text'])
            combined_score = self.calculate_sentence_score(current_segment['text'] + ' ' + text)
            
            # åˆå¹¶åˆ¤æ–­é€»è¾‘
            should_merge = False
            
            # 1. åŸºç¡€æ¡ä»¶ï¼šæ—¶é—´é—´éš”å’Œæœ€å¤§é•¿åº¦é™åˆ¶
            if gap <= 2.0 and new_duration <= max_duration:
                
                # 2. å¦‚æœå½“å‰ç‰‡æ®µæœªå®Œæˆå¥å­ï¼Œå€¾å‘äºåˆå¹¶
                if current_score < 8:
                    should_merge = True
                
                # 3. å¦‚æœåˆå¹¶åå¥å­æ›´å®Œæ•´
                elif combined_score > current_score + 3:
                    should_merge = True
                
                # 4. å½“å‰ç‰‡æ®µå¤ªçŸ­ï¼Œéœ€è¦åˆå¹¶
                elif current_duration < 8.0:
                    should_merge = True
                
                # 5. ç›®æ ‡æ—¶é•¿å†…ä¸”å¥å­ä¸å®Œæ•´
                elif current_duration < target_duration and not self.is_sentence_end(current_segment['text']):
                    should_merge = True
            
            # æ‰§è¡Œåˆå¹¶æˆ–åˆ†å‰²
            if should_merge:
                # åˆå¹¶ç‰‡æ®µ
                current_segment['end'] = end
                current_segment['text'] += ' ' + text
                current_segment['original_segments'].append(segment)
            else:
                # åˆ†å‰²ï¼šä¿å­˜å½“å‰ç‰‡æ®µï¼Œå¼€å§‹æ–°ç‰‡æ®µ
                merged_segments.append(current_segment)
                current_segment = {
                    'start': start,
                    'end': end,
                    'text': text,
                    'original_segments': [segment]
                }
        
        # ä¿å­˜æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_segment is not None:
            merged_segments.append(current_segment)
        
        # ç»Ÿè®¡ä¿¡æ¯
        avg_duration = sum(seg['end'] - seg['start'] for seg in merged_segments) / len(merged_segments) if merged_segments else 0
        complete_sentences = sum(1 for seg in merged_segments if self.is_sentence_end(seg['text']))
        
        self.log(f"ğŸ“Š å­—å¹•åˆå¹¶å®Œæˆ:")
        self.log(f"   åŸå§‹ç‰‡æ®µ: {len(segments)} â†’ åˆå¹¶å: {len(merged_segments)}")
        self.log(f"   å¹³å‡æ—¶é•¿: {avg_duration:.1f}ç§’")
        self.log(f"   å®Œæ•´å¥å­: {complete_sentences}/{len(merged_segments)} ({complete_sentences/len(merged_segments)*100:.1f}%)")
        
        return merged_segments

    def parse_srt_file(self, srt_file):
        """è§£æSRTæ–‡ä»¶è·å–segmentsä¿¡æ¯"""
        segments = []
        try:
            with open(srt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•çš„SRTè§£æ
            blocks = content.strip().split('\n\n')
            for block in blocks:
                lines = block.strip().split('\n')
                if len(lines) >= 3:
                    # è§£ææ—¶é—´æˆ³
                    time_line = lines[1]
                    if ' --> ' in time_line:
                        start_str, end_str = time_line.split(' --> ')
                        start_seconds = self.srt_time_to_seconds(start_str)
                        end_seconds = self.srt_time_to_seconds(end_str)
                        
                        # åˆå¹¶æ–‡æœ¬è¡Œ
                        text = ' '.join(lines[2:])
                        
                        segments.append({
                            'start': start_seconds,
                            'end': end_seconds,
                            'text': text
                        })
            
            return segments
        except Exception as e:
            print(f"è§£æSRTæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def srt_time_to_seconds(self, time_str):
        """å°†SRTæ—¶é—´æ ¼å¼è½¬æ¢ä¸ºç§’æ•°"""
        try:
            # æ ¼å¼: HH:MM:SS,mmm
            time_part, ms_part = time_str.split(',')
            h, m, s = map(int, time_part.split(':'))
            ms = int(ms_part)
            return h * 3600 + m * 60 + s + ms / 1000.0
        except:
            return 0
    
    def generate_srt(self, segments):
        """ç”ŸæˆSRTæ ¼å¼å­—å¹•"""
        srt_content = ""
        for i, segment in enumerate(segments):
            start_time = self.seconds_to_srt_time(segment['start'])
            end_time = self.seconds_to_srt_time(segment['end'])
            text = segment['text'].strip()
            
            srt_content += f"{i+1}\n"
            srt_content += f"{start_time} --> {end_time}\n"
            srt_content += f"{text}\n\n"
        
        return srt_content
    
    def seconds_to_srt_time(self, seconds):
        """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    def analyze_content(self, transcript, segments):
        """ä½¿ç”¨AIåˆ†æå†…å®¹å¹¶ç”Ÿæˆç®€æŠ¥"""
        try:
            # æ›´å‡†ç¡®çš„tokenä¼°ç®— (ä¸­æ–‡: 1å­—ç¬¦ â‰ˆ 1.5 tokens, è‹±æ–‡: 1 token â‰ˆ 4 characters)
            # ä¸ºä¸­æ–‡å†…å®¹ä½¿ç”¨æ›´ä¿å®ˆçš„ä¼°ç®—
            estimated_tokens = len(transcript) * 1.5  # ä¸­æ–‡å­—ç¬¦æ›´å‡†ç¡®çš„tokenä¼°ç®—
            
            # GPT-4çš„å®é™…é™åˆ¶ï¼šè¾“å…¥tokençº¦8192ï¼Œéœ€è¦é¢„ç•™è¾“å‡ºç©ºé—´
            # æç¤ºè¯å¤§çº¦ä½¿ç”¨500-800 tokensï¼Œè¾“å‡ºéœ€è¦é¢„ç•™1000-1500 tokens
            max_input_tokens = 6000  # ä¿å®ˆä¼°è®¡ï¼Œç¡®ä¿ä¸è¶…è¿‡GPT-4é™åˆ¶
            
            self.log(f"ğŸ“Š æ–‡å­—ç¨¿é•¿åº¦: {len(transcript)} å­—ç¬¦")
            self.log(f"ğŸ“Š ä¼°ç®—tokenæ•°: {estimated_tokens:.0f}")
            self.log(f"ğŸ“Š æ¨¡å‹é™åˆ¶: {max_input_tokens} tokens (åŒ…å«æç¤ºè¯)")
            
            if estimated_tokens <= max_input_tokens:
                self.log("ğŸ“ æ–‡æœ¬é•¿åº¦é€‚ä¸­ï¼Œä½¿ç”¨å•æ¬¡åˆ†æ")
                return self._analyze_single_chunk(transcript, segments)
            else:
                self.log("ğŸ“ æ–‡æœ¬è¿‡é•¿ï¼Œä½¿ç”¨åˆ†æ®µåˆ†æ")
                return self._analyze_multiple_chunks(transcript, segments, max_input_tokens)
            
        except Exception as e:
            raise Exception(f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}")

    def _format_segments_for_gpt(self, segments):
        """å°†segmentsæ ¼å¼åŒ–ä¸ºå¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬ï¼Œä¾›GPTç›´æ¥åˆ†æ"""
        formatted_lines = []
        
        for segment in segments:
            start_time = segment.get('start', 0)
            text = segment.get('text', '').strip()
            
            if text:
                # å°†ç§’æ•°è½¬æ¢ä¸º mm:ss æ ¼å¼
                minutes = int(start_time // 60)
                seconds = int(start_time % 60)
                time_str = f"{minutes:02d}:{seconds:02d}"
                
                formatted_lines.append(f"[{time_str}] {text}")
        
        return '\n'.join(formatted_lines)

    def _analyze_single_chunk(self, transcript, segments):
        """åˆ†æå•ä¸ªæ–‡æœ¬å— - é‡æ„ç‰ˆï¼šåŸºäºå¸¦æ—¶é—´æˆ³çš„å­—å¹•ç›´æ¥åˆ†æ"""
        # æ ¼å¼åŒ–segmentsä¸ºå¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬
        timestamped_content = self._format_segments_for_gpt(segments)
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹YouTubeè§†é¢‘çš„å¸¦æ—¶é—´æˆ³å­—å¹•ï¼Œå¹¶ç”Ÿæˆä¸€ä»½ç®€æŠ¥ã€‚

å¸¦æ—¶é—´æˆ³çš„å­—å¹•å†…å®¹ï¼š
{timestamped_content}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºJSONï¼š
{{
    "summary": "è§†é¢‘ä¸»è¦å†…å®¹çš„ç®€æ´æ€»ç»“ï¼ˆ3-5å¥è¯ï¼‰",
    "key_points": [
        {{
            "point": "è¦ç‚¹æè¿°",
            "explanation": "è¯¦ç»†è§£é‡Š",
            "timestamp": 83,
            "quote": "åŸæ–‡å¼•ç”¨"
        }}
    ]
}}

é‡è¦è¦æ±‚ï¼š
1. æå–3-8ä¸ªå…³é”®è¦ç‚¹ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
2. timestampå­—æ®µå¿…é¡»å¡«å†™å‡†ç¡®çš„ç§’æ•°ï¼ˆä»å­—å¹•çš„æ—¶é—´æˆ³ä¸­è·å–ï¼‰
3. å¦‚æœæ˜¯æ®µè½æ€»ç»“ï¼Œä½¿ç”¨è¯¥æ®µè½ç¬¬ä¸€ä¸ªå­—å¹•çš„æ—¶é—´æˆ³
4. å¦‚æœå¼•ç”¨äº†æŸå¥é‡‘å¥ï¼Œä½¿ç”¨è¯¥é‡‘å¥æ‰€åœ¨å­—å¹•çš„å‡†ç¡®æ—¶é—´æˆ³
5. ä¸åŒè¦ç‚¹å¿…é¡»æœ‰ä¸åŒçš„æ—¶é—´æˆ³ï¼Œä½“ç°å†…å®¹çš„æ—¶é—´è¿›å±•
6. quoteå­—æ®µåŒ…å«ç›¸å…³çš„åŸæ–‡ç‰‡æ®µï¼Œä½†ä¸ç”¨äºåŒ¹é…ï¼ˆæ—¶é—´æˆ³å·²ç»å‡†ç¡®ï¼‰

ç¤ºä¾‹æ—¶é—´æˆ³æ ¼å¼ï¼šå¦‚æœå­—å¹•æ˜¾ç¤º[02:35]ï¼Œåˆ™timestampåº”è¯¥å¡«å†™155ï¼ˆ2åˆ†35ç§’=155ç§’ï¼‰
"""

        try:
            self.log("ğŸ¤– å‘é€GPTè¯·æ±‚...")
            response = self.openai_client.chat.completions.create(
                model="gpt-4",  # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1500  # é™åˆ¶è¾“å‡ºtokenæ•°é‡
            )
            self.log("âœ… GPTè¯·æ±‚æˆåŠŸ")
        except Exception as e:
            self.log(f"âŒ GPTè¯·æ±‚å¤±è´¥: {str(e)}")
            # å¦‚æœé‡åˆ°tokené™åˆ¶é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨æ›´å¤§å®¹é‡çš„æ¨¡å‹
            if "token" in str(e).lower() or "context" in str(e).lower():
                self.log(f"âš ï¸ GPT-4 tokené™åˆ¶ï¼Œå°è¯•ä½¿ç”¨gpt-4-turbo...")
                try:
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4-turbo",
                        messages=[{"role": "user", "content": prompt}],
                        temperature=0.3,
                        max_tokens=1500
                    )
                    self.log("âœ… gpt-4-turboè¯·æ±‚æˆåŠŸ")
                except Exception as e2:
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ç¼©çŸ­æ–‡æœ¬
                    self.log(f"âš ï¸ gpt-4-turboä¹Ÿå¤±è´¥ï¼Œç¼©çŸ­æ–‡æœ¬é‡è¯•...")
                    try:
                        # ç¼©çŸ­å†…å®¹é‡è¯•
                        shortened_content = timestamped_content[:3000]  # ç¼©çŸ­æ—¶é—´æˆ³å†…å®¹
                        shortened_prompt = prompt.replace(timestamped_content, shortened_content)
                        response = self.openai_client.chat.completions.create(
                            model="gpt-4",
                            messages=[{"role": "user", "content": shortened_prompt}],
                            temperature=0.3,
                            max_tokens=1500
                        )
                        self.log("âœ… ç¼©çŸ­å†…å®¹åè¯·æ±‚æˆåŠŸ")
                    except Exception as e3:
                        self.log(f"âŒ æ‰€æœ‰GPTé‡è¯•éƒ½å¤±è´¥: {str(e3)}")
                        return self._generate_fallback_analysis(transcript, segments)
            elif "rate" in str(e).lower() or "quota" in str(e).lower():
                self.log(f"âš ï¸ APIé…é¢æˆ–é€Ÿç‡é™åˆ¶ï¼Œç”Ÿæˆå¤‡ç”¨ç®€æŠ¥")
                return self._generate_fallback_analysis(transcript, segments)
            elif "api" in str(e).lower() or "network" in str(e).lower() or "connection" in str(e).lower():
                self.log(f"âš ï¸ ç½‘ç»œæˆ–APIè¿æ¥é—®é¢˜ï¼Œç”Ÿæˆå¤‡ç”¨ç®€æŠ¥")
                return self._generate_fallback_analysis(transcript, segments)
            else:
                self.log(f"âš ï¸ æœªçŸ¥GPTé”™è¯¯ï¼Œç”Ÿæˆå¤‡ç”¨ç®€æŠ¥")
                return self._generate_fallback_analysis(transcript, segments)
        
        # æ·»åŠ GPTå“åº”è°ƒè¯•ä¿¡æ¯
        gpt_response = response.choices[0].message.content
        self.log(f"ğŸ¤– GPTå“åº”å†…å®¹é•¿åº¦: {len(gpt_response) if gpt_response else 0}")
        
        if not gpt_response or gpt_response.strip() == "":
            self.log("âŒ GPTè¿”å›ç©ºå“åº”ï¼Œç”Ÿæˆé»˜è®¤ç®€æŠ¥")
            # ç”ŸæˆåŸºç¡€ç®€æŠ¥ä½œä¸ºfallback
            return self._generate_fallback_analysis(transcript, segments)
        
        # æ‰“å°å‰200ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
        self.log(f"ğŸ” GPTå“åº”å‰200å­—ç¬¦: {gpt_response[:200]}")
        
        try:
            analysis_result = json.loads(gpt_response)
        except json.JSONDecodeError as e:
            self.log(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
            self.log(f"ğŸ” GPTå®Œæ•´å“åº”: {gpt_response}")
            
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
            cleaned_response = gpt_response.strip()
            
            # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response.replace("```json", "").replace("```", "").strip()
                self.log(f"ğŸ”§ å°è¯•ç§»é™¤ä»£ç å—æ ‡è®°")
            elif cleaned_response.startswith("```"):
                cleaned_response = cleaned_response.replace("```", "").strip()
                self.log(f"ğŸ”§ å°è¯•ç§»é™¤é€šç”¨ä»£ç å—æ ‡è®°")
            
            # ç§»é™¤å¯èƒ½çš„å‰ç¼€æ–‡æœ¬
            if "{" in cleaned_response:
                json_start = cleaned_response.find("{")
                cleaned_response = cleaned_response[json_start:]
                self.log(f"ğŸ”§ å°è¯•ç§»é™¤JSONå‰çš„æ–‡æœ¬")
            
            # ç§»é™¤å¯èƒ½çš„åç¼€æ–‡æœ¬
            if "}" in cleaned_response:
                json_end = cleaned_response.rfind("}") + 1
                cleaned_response = cleaned_response[:json_end]
                self.log(f"ğŸ”§ å°è¯•ç§»é™¤JSONåçš„æ–‡æœ¬")
            
            try:
                analysis_result = json.loads(cleaned_response)
                self.log(f"âœ… JSONä¿®å¤æˆåŠŸ")
            except json.JSONDecodeError as e2:
                self.log(f"âŒ JSONä¿®å¤å¤±è´¥: {str(e2)}")
                self.log(f"ğŸ”§ ç”Ÿæˆå¤‡ç”¨ç®€æŠ¥")
                # å¦‚æœJSONå®Œå…¨æ— æ³•è§£æï¼Œç”Ÿæˆfallback
                return self._generate_fallback_analysis(transcript, segments)
        
        # æ–°æµç¨‹ï¼šGPTç›´æ¥è¿”å›å‡†ç¡®çš„æ—¶é—´æˆ³ï¼Œæ— éœ€åç»­åŒ¹é…
        # éªŒè¯æ—¶é—´æˆ³çš„åˆç†æ€§
        if 'key_points' in analysis_result:
            for i, point in enumerate(analysis_result['key_points']):
                timestamp = point.get('timestamp', 0)
                # ç¡®ä¿æ—¶é—´æˆ³æ˜¯æ•°å­—ä¸”åˆç†
                if not isinstance(timestamp, (int, float)) or timestamp < 0:
                    self.log(f"âš ï¸ è¦ç‚¹{i+1}çš„æ—¶é—´æˆ³æ— æ•ˆ: {timestamp}ï¼Œè®¾ä¸º0")
                    point['timestamp'] = 0
                else:
                    self.log(f"âœ… è¦ç‚¹{i+1}æ—¶é—´æˆ³: {timestamp}ç§’ ({int(timestamp//60):02d}:{int(timestamp%60):02d})")
        
        return analysis_result

    def _generate_fallback_analysis(self, transcript, segments):
        """å½“GPTå¤±è´¥æ—¶ç”ŸæˆåŸºç¡€ç®€æŠ¥ä½œä¸ºfallback"""
        self.log("ğŸ”§ ç”Ÿæˆå¤‡ç”¨åˆ†æç®€æŠ¥...")
        
        # ç®€å•çš„æ–‡æœ¬ç»Ÿè®¡åˆ†æ
        total_duration = segments[-1].get('end', 0) if segments else 0
        total_segments = len(segments)
        
        # åŸºç¡€æ‘˜è¦
        summary = f"è¯¥è§†é¢‘æ—¶é•¿çº¦{int(total_duration//60)}åˆ†{int(total_duration%60)}ç§’ï¼Œå…±åŒ…å«{total_segments}æ®µå­—å¹•å†…å®¹ã€‚"
        
        # ç”ŸæˆåŸºç¡€è¦ç‚¹ï¼ˆåŸºäºæ—¶é—´åˆ†æ®µï¼‰
        key_points = []
        if segments:
            # å°†è§†é¢‘åˆ†æˆ3-5ä¸ªæ—¶é—´æ®µ
            num_points = min(5, max(3, total_segments // 10))
            segment_size = len(segments) // num_points
            
            for i in range(num_points):
                start_idx = i * segment_size
                end_idx = min((i + 1) * segment_size, len(segments))
                
                if start_idx < len(segments):
                    segment_text = ' '.join([seg.get('text', '') for seg in segments[start_idx:end_idx]])
                    segment_start = segments[start_idx].get('start', 0)
                    
                    # æˆªå–å‰100å­—ç¬¦ä½œä¸ºè¦ç‚¹
                    point_text = segment_text[:100] + ("..." if len(segment_text) > 100 else "")
                    
                    key_points.append({
                        "point": f"ç¬¬{i+1}æ®µå†…å®¹è¦ç‚¹",
                        "explanation": point_text,
                        "timestamp": int(segment_start),
                        "quote": segment_text[:200] + ("..." if len(segment_text) > 200 else "")
                    })
        
        if not key_points:
            # å¦‚æœæ²¡æœ‰segmentsï¼Œç”Ÿæˆä¸€ä¸ªé»˜è®¤è¦ç‚¹
            key_points.append({
                "point": "è§†é¢‘å†…å®¹",
                "explanation": "è§†é¢‘åŒ…å«è¯­éŸ³å†…å®¹ï¼Œä½†è‡ªåŠ¨åˆ†æå¤±è´¥ã€‚",
                "timestamp": 0,
                "quote": transcript[:200] + ("..." if len(transcript) > 200 else "")
            })
        
        return {
            "summary": summary,
            "key_points": key_points
        }

    def _analyze_multiple_chunks(self, transcript, segments, max_input_tokens):
        """åˆ†æ®µåˆ†æé•¿æ–‡æœ¬"""
        # è½¬æ¢tokené™åˆ¶ä¸ºå­—ç¬¦æ•°ï¼ˆä¸­æ–‡å­—ç¬¦ï¼‰
        # ä¸ºåˆ†æ®µé¢„ç•™ä¸€äº›tokenç©ºé—´ç»™æç¤ºè¯
        prompt_tokens = 500  # é¢„ç•™ç»™æç¤ºè¯çš„token
        available_tokens = max_input_tokens - prompt_tokens
        chunk_size_chars = int(available_tokens / 1.5)  # è½¬æ¢ä¸ºä¸­æ–‡å­—ç¬¦æ•°
        
        chunks = []
        
        # æ™ºèƒ½åˆ†å‰²ï¼šå…ˆå°è¯•å¥å­è¾¹ç•Œï¼Œå¦‚æœæ²¡æœ‰åˆ™æŒ‰å­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²
        # å°è¯•ä¸åŒçš„åˆ†å‰²æ–¹æ³•
        potential_delimiters = ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n', ' ']
        best_sentences = None
        
        for delimiter in potential_delimiters:
            test_sentences = transcript.split(delimiter)
            if len(test_sentences) > 1:  # æ‰¾åˆ°æœ‰æ•ˆåˆ†å‰²
                best_sentences = test_sentences
                best_delimiter = delimiter
                break
        
        if best_sentences is None or len(best_sentences) == 1:
            # æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„åˆ†éš”ç¬¦ï¼ŒæŒ‰å­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²
            best_sentences = []
            for i in range(0, len(transcript), chunk_size_chars):
                chunk = transcript[i:i + chunk_size_chars]
                best_sentences.append(chunk)
            best_delimiter = ""
        
        current_chunk = ""
        current_segments = []
        
        for i, sentence in enumerate(best_sentences):
            # é‡æ–°åŠ ä¸Šåˆ†éš”ç¬¦ï¼ˆé™¤äº†æœ€åä¸€å¥å’Œå¼ºåˆ¶åˆ†å‰²çš„æƒ…å†µï¼‰
            if best_delimiter and i < len(best_sentences) - 1:
                sentence_with_delimiter = sentence + best_delimiter
            else:
                sentence_with_delimiter = sentence
            
            # æ£€æŸ¥æ·»åŠ è¿™ä¸ªå¥å­æ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
            if len(current_chunk + sentence_with_delimiter) <= chunk_size_chars or not current_chunk:
                current_chunk += sentence_with_delimiter
                # æ‰¾åˆ°å¯¹åº”çš„segments
                chunk_segments = [s for s in segments if sentence[:20] in s.get('text', '')]
                current_segments.extend(chunk_segments)
            else:
                # å½“å‰å¥å­ä¼šå¯¼è‡´è¶…é™ï¼Œä¿å­˜å½“å‰å—å¹¶å¼€å§‹æ–°å—
                if current_chunk:  # ç¡®ä¿ä¸ä¿å­˜ç©ºå—
                    chunks.append((current_chunk, current_segments))
                
                # æ£€æŸ¥å•ä¸ªå¥å­æ˜¯å¦å¤ªé•¿
                if len(sentence_with_delimiter) > chunk_size_chars:
                    # å¥å­å¤ªé•¿ï¼ŒæŒ‰å­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²
                    for j in range(0, len(sentence_with_delimiter), chunk_size_chars):
                        sub_chunk = sentence_with_delimiter[j:j + chunk_size_chars]
                        if sub_chunk:
                            chunks.append((sub_chunk, []))
                    current_chunk = ""
                    current_segments = []
                else:
                    current_chunk = sentence_with_delimiter
                    current_segments = [s for s in segments if sentence[:20] in s.get('text', '')]
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append((current_chunk, current_segments))
        
        self.log(f"ğŸ“ åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—è¿›è¡Œåˆ†æ")
        self.log(f"ğŸ“ æ¯å—æœ€å¤§å­—ç¬¦æ•°: {chunk_size_chars}")
        
        # åˆ†ææ¯ä¸ªchunk
        all_summaries = []
        all_key_points = []
        
        for i, (chunk_text, chunk_segments) in enumerate(chunks):
            chunk_char_count = len(chunk_text)
            estimated_chunk_tokens = chunk_char_count * 1.5
            self.log(f"ğŸ“Š åˆ†æç¬¬ {i+1}/{len(chunks)} ä¸ªæ–‡æœ¬å— ({chunk_char_count}å­—ç¬¦, ~{estimated_chunk_tokens:.0f}tokens)...")
            
            try:
                chunk_analysis = self._analyze_chunk_with_context(chunk_text, i+1, len(chunks))
                
                if 'summary' in chunk_analysis:
                    all_summaries.append(chunk_analysis['summary'])
                if 'key_points' in chunk_analysis:
                    # è°ƒæ•´æ—¶é—´æˆ³ä¸ºåŸè§†é¢‘çš„ç›¸å¯¹æ—¶é—´
                    adjusted_points = []
                    for point in chunk_analysis['key_points']:
                        # åœ¨åŸsegmentsä¸­æ‰¾åˆ°åŒ¹é…çš„æ—¶é—´æˆ³
                        matching_segment = self._find_matching_segment(point.get('quote', ''), segments)
                        if matching_segment:
                            point['timestamp'] = matching_segment['start']
                        adjusted_points.append(point)
                    all_key_points.extend(adjusted_points)
            except Exception as e:
                self.log(f"âš ï¸ ç¬¬{i+1}å—åˆ†æå¤±è´¥: {str(e)}")
                # ç»§ç»­å¤„ç†å…¶ä»–å—
                continue
        
        # åˆå¹¶æ‰€æœ‰åˆ†æç»“æœ
        self.log("ğŸ“Š åˆå¹¶åˆ†æç»“æœ...")
        final_summary = self._merge_summaries(all_summaries)
        final_key_points = self._merge_key_points(all_key_points)
        
        return {
            'summary': final_summary,
            'key_points': final_key_points
        }

    def _analyze_chunk_with_context(self, chunk_text, chunk_index, total_chunks):
        """åˆ†æå•ä¸ªæ–‡æœ¬å—ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰"""
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹YouTubeè§†é¢‘çš„éƒ¨åˆ†æ–‡å­—ç¨¿ï¼ˆç¬¬{chunk_index}éƒ¨åˆ†ï¼Œå…±{total_chunks}éƒ¨åˆ†ï¼‰ï¼š

æ–‡å­—ç¨¿å†…å®¹ï¼š
{chunk_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºJSONï¼š
{{
    "summary": "è¿™éƒ¨åˆ†å†…å®¹çš„ç®€æ´æ€»ç»“ï¼ˆ2-3å¥è¯ï¼‰",
    "key_points": [
        {{
            "point": "è¦ç‚¹æè¿°",
            "explanation": "è¯¦ç»†è§£é‡Š",
            "timestamp": 0,
            "quote": "åŸæ–‡å¼•ç”¨ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰"
        }}
    ]
}}

è¦æ±‚ï¼š
1. æå–2-4ä¸ªå…³é”®è¦ç‚¹
2. é‡ç‚¹å…³æ³¨è¿™éƒ¨åˆ†çš„ä¸»è¦è§‚ç‚¹
3. æä¾›åŸæ–‡å¼•ç”¨ä»¥ä¾¿åç»­åŒ¹é…æ—¶é—´æˆ³
"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1200  # åˆ†å—åˆ†æä½¿ç”¨è¾ƒå°‘çš„è¾“å‡ºtoken
            )
        except Exception as e:
            self.log(f"âŒ GPTåˆ†å—è¯·æ±‚å¤±è´¥: {str(e)}")
            if "token" in str(e).lower() or "context" in str(e).lower():
                # å¦‚æœchunkä»ç„¶å¤ªå¤§ï¼Œè¿›ä¸€æ­¥ç¼©çŸ­
                self.log("âš ï¸ è¿›ä¸€æ­¥ç¼©çŸ­æ–‡æœ¬é‡è¯•...")
                shortened_chunk = chunk_text[:2000]
                shortened_prompt = prompt.replace(chunk_text, shortened_chunk)
                try:
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4",
                        messages=[{"role": "user", "content": shortened_prompt}],
                        temperature=0.3,
                        max_tokens=1200
                    )
                except Exception as e2:
                    self.log(f"âŒ ç¼©çŸ­æ–‡æœ¬åä»å¤±è´¥: {str(e2)}")
                    # è¿”å›ç©ºç»“æœ
                    return {"summary": "åˆ†å—åˆ†æå¤±è´¥", "key_points": []}
            else:
                self.log(f"âŒ å…¶ä»–GPTé”™è¯¯: {str(e)}")
                return {"summary": "åˆ†å—åˆ†æå¤±è´¥", "key_points": []}
        
        # å®‰å…¨çš„JSONè§£æ
        try:
            gpt_response = response.choices[0].message.content
            if not gpt_response or gpt_response.strip() == "":
                self.log("âŒ GPTåˆ†å—è¿”å›ç©ºå“åº”")
                return {"summary": "åˆ†å—åˆ†æè¿”å›ç©ºå“åº”", "key_points": []}
            
            return json.loads(gpt_response)
        except json.JSONDecodeError as e:
            self.log(f"âŒ GPTåˆ†å—JSONè§£æå¤±è´¥: {str(e)}")
            # è¿”å›åŸºç¡€ç»“æœ
            return {
                "summary": f"æ­¤éƒ¨åˆ†åŒ…å«{len(chunk_text)}å­—ç¬¦çš„å†…å®¹",
                "key_points": [{
                    "point": "å†…å®¹ç‰‡æ®µ",
                    "explanation": chunk_text[:100] + "...",
                    "quote": chunk_text[:200] + "..."
                }]
            }

    def _find_matching_segment(self, quote_text, segments):
        """åœ¨segmentsä¸­æ‰¾åˆ°åŒ¹é…çš„æ–‡æœ¬ç‰‡æ®µï¼Œä½¿ç”¨æ”¹è¿›çš„åŒ¹é…ç®—æ³•"""
        if not quote_text or not segments:
            return None
        
        # æ¸…ç†å¼•ç”¨æ–‡æœ¬
        quote_clean = self._clean_text_for_matching(quote_text)
        if not quote_clean:
            return None
        
        best_match = None
        best_score = 0
        
        # é¦–å…ˆå°è¯•ç²¾ç¡®å­å­—ç¬¦ä¸²åŒ¹é…ï¼ˆä¸åŒºåˆ†å¤§å°å†™å’Œæ ‡ç‚¹ï¼‰
        for segment in segments:
            segment_text = segment.get('text', '').lower()
            quote_lower = quote_text.lower()
            
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·è¿›è¡ŒåŒ¹é…
            import re
            segment_clean_simple = re.sub(r'[^\w\s]', '', segment_text)
            quote_clean_simple = re.sub(r'[^\w\s]', '', quote_lower)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿé•¿çš„å…±åŒå­å­—ç¬¦ä¸²
            if len(quote_clean_simple) > 10:  # å¼•ç”¨è¶³å¤Ÿé•¿
                if quote_clean_simple in segment_clean_simple or segment_clean_simple in quote_clean_simple:
                    self.log(f"ğŸ¯ æ—¶é—´æˆ³åŒ¹é…: æ‰¾åˆ°å­å­—ç¬¦ä¸²ç²¾ç¡®åŒ¹é…")
                    return segment
        
        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•åˆ†è¯åŒ¹é…
        quote_words = quote_clean.split()
        if len(quote_words) >= 3:  # è‡³å°‘3ä¸ªè¯æ‰è¿›è¡ŒåŒ¹é…
            for segment in segments:
                # ä¼˜å…ˆåœ¨åˆå¹¶ç‰‡æ®µçš„åŸå§‹ç‰‡æ®µä¸­æŸ¥æ‰¾æ›´ç²¾ç¡®çš„åŒ¹é…
                if 'original_segments' in segment and segment['original_segments']:
                    for orig_segment in segment['original_segments']:
                        orig_clean = self._clean_text_for_matching(orig_segment.get('text', ''))
                        if orig_clean:
                            score = self._calculate_word_overlap(quote_words, orig_clean.split())
                            if score > best_score:
                                best_score = score
                                best_match = orig_segment
                
                # ä¹Ÿæ£€æŸ¥åˆå¹¶åçš„ç‰‡æ®µ
                segment_clean = self._clean_text_for_matching(segment.get('text', ''))
                if segment_clean:
                    score = self._calculate_word_overlap(quote_words, segment_clean.split())
                    if score > best_score:
                        best_score = score
                        best_match = segment
        
            # é™ä½åŒ¹é…é˜ˆå€¼ï¼Œå› ä¸ºåˆ†è¯åŒ¹é…æ›´å¯é 
            if best_score >= 0.2:  # 20%çš„è¯æ±‡é‡å é˜ˆå€¼ï¼ˆä»40%é™ä½ï¼‰
                self.log(f"ğŸ¯ æ—¶é—´æˆ³åŒ¹é…: æ‰¾åˆ°{best_score:.2f}è¯æ±‡é‡å åŒ¹é…")
                return best_match
        
        # æœ€åå°è¯•éƒ¨åˆ†åŒ¹é…
        partial_match = self._find_partial_match(quote_clean, segments)
        if partial_match:
            self.log(f"âš ï¸ æ—¶é—´æˆ³åŒ¹é…: ä½¿ç”¨éƒ¨åˆ†åŒ¹é…")
            return partial_match
        
        # æœ€åçš„å›é€€é€‰é¡¹ - æ™ºèƒ½ä½ç½®ä¼°ç®—
        if segments:
            # æ”¹è¿›çš„å¯å‘å¼ï¼šæ ¹æ®å¼•ç”¨æ–‡æœ¬åœ¨å®Œæ•´è½¬å½•ä¸­çš„ä½ç½®ä¼°ç®—æ—¶é—´æˆ³
            estimated_position = self._estimate_quote_position(quote_text, segments)
            if estimated_position is not None:
                self.log(f"ğŸ“ æ—¶é—´æˆ³åŒ¹é…: ä½¿ç”¨ä½ç½®ä¼°ç®—åŒ¹é…")
                return estimated_position
            
            # æ™ºèƒ½å›é€€ç­–ç•¥ï¼šä¸æ€»æ˜¯ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰‡æ®µ
            fallback_segment = self._get_fallback_segment(segments)
            self.log(f"âŒ æ—¶é—´æˆ³åŒ¹é…: æœªæ‰¾åˆ°åŒ¹é…ï¼Œä½¿ç”¨æ™ºèƒ½å›é€€ç­–ç•¥")
            return fallback_segment
        
        return None
    
    def _calculate_word_overlap(self, words1, words2):
        """è®¡ç®—ä¸¤ä¸ªè¯åˆ—è¡¨çš„é‡å ç‡"""
        if not words1 or not words2:
            return 0
        
        set1 = set(words1)
        set2 = set(words2)
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0
    
    def _estimate_quote_position(self, quote_text, segments):
        """æ ¹æ®å¼•ç”¨æ–‡æœ¬ä¼°ç®—åœ¨segmentsä¸­çš„ä½ç½® - æ”¹è¿›ç‰ˆæœ¬"""
        if not quote_text or not segments:
            return None
        
        # æ„å»ºå®Œæ•´æ–‡æœ¬å’Œä½ç½®æ˜ å°„
        full_text_parts = []
        segment_boundaries = []  # è®°å½•æ¯ä¸ªsegmentåœ¨å®Œæ•´æ–‡æœ¬ä¸­çš„è¾¹ç•Œ
        
        current_pos = 0
        for i, seg in enumerate(segments):
            seg_text = seg.get('text', '')
            full_text_parts.append(seg_text)
            segment_boundaries.append((current_pos, current_pos + len(seg_text), i))
            current_pos += len(seg_text) + 1  # +1 for space
        
        full_text = ' '.join(full_text_parts)
        
        # æŸ¥æ‰¾å¼•ç”¨åœ¨å®Œæ•´æ–‡æœ¬ä¸­çš„å¤§è‡´ä½ç½®
        quote_clean = quote_text.lower()
        full_text_clean = full_text.lower()
        
        # å°è¯•æ‰¾åˆ°å¼•ç”¨çš„å…³é”®è¯åœ¨å…¨æ–‡ä¸­çš„ä½ç½®
        quote_words = [w for w in quote_clean.split() if len(w) > 2][:8]  # å–å‰8ä¸ªæœ‰æ„ä¹‰çš„è¯
        
        if not quote_words:
            return self._get_fallback_segment(segments)
        
        best_segment = None
        max_score = 0
        
        # ä¸ºæ¯ä¸ªsegmentè®¡ç®—åŒ¹é…åˆ†æ•°
        for start_pos, end_pos, seg_idx in segment_boundaries:
            if seg_idx >= len(segments):
                continue
                
            # æ‰©å±•çª—å£ï¼šåŒ…å«å½“å‰segmentåŠå…¶å‘¨å›´çš„æ–‡æœ¬
            window_start = max(0, start_pos - 100)
            window_end = min(len(full_text_clean), end_pos + 100)
            window_text = full_text_clean[window_start:window_end]
            
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            word_score = sum(1 for word in quote_words if word in window_text)
            
            # å½’ä¸€åŒ–åˆ†æ•°
            normalized_score = word_score / len(quote_words) if quote_words else 0
            
            if normalized_score > max_score:
                max_score = normalized_score
                best_segment = segments[seg_idx]
        
        if best_segment and max_score >= 0.25:  # è‡³å°‘25%çš„å…³é”®è¯åŒ¹é…
            self.log(f"ğŸ“ ä½ç½®ä¼°ç®—: æ‰¾åˆ° {max_score:.2f} åŒ¹é…åˆ†æ•°")
            return best_segment
        
        # æœ€åçš„å›é€€ç­–ç•¥
        return self._get_fallback_segment(segments)
    
    def correct_transcript_with_gpt(self, transcript_text, language='zh'):
        """ä½¿ç”¨GPTè¿›è¡Œæ™ºèƒ½å­—å¹•æ ¡æ­£å’Œæ–­å¥ä¼˜åŒ–"""
        try:
            self.log("ğŸ” å¼€å§‹GPTæ™ºèƒ½å­—å¹•æ ¡æ­£...")
            
            # åˆ†æ®µå¤„ç†ï¼Œé¿å…tokené™åˆ¶
            max_chars_per_chunk = 1800
            chunks = self._split_text_for_correction(transcript_text, max_chars_per_chunk)
            
            corrected_chunks = []
            total_corrections = 0
            
            for i, chunk in enumerate(chunks):
                self.log(f"ğŸ“ æ ¡æ­£ç¬¬ {i+1}/{len(chunks)} æ®µæ–‡æœ¬...")
                
                corrected_chunk, corrections = self._correct_text_chunk(chunk, language)
                corrected_chunks.append(corrected_chunk)
                total_corrections += corrections
            
            # åˆå¹¶æ ¡æ­£åçš„æ–‡æœ¬
            corrected_transcript = ' '.join(corrected_chunks)
            
            # è®¡ç®—æ”¹è¿›è¯„åˆ†
            quality_score = self._calculate_text_quality_score(corrected_transcript)
            
            self.log(f"âœ… GPTå­—å¹•æ ¡æ­£å®Œæˆ:")
            self.log(f"   æ€»è®¡ä¿®æ­£: {total_corrections} å¤„")
            self.log(f"   è´¨é‡è¯„åˆ†: {quality_score:.1f}/10")
            
            return corrected_transcript
            
        except Exception as e:
            self.log(f"âŒ GPTå­—å¹•æ ¡æ­£å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨åŸå§‹è½¬å½•")
            return transcript_text
    
    def _split_text_for_correction(self, text, max_chars):
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ç”¨äºæ ¡æ­£"""
        chunks = []
        
        # ä¼˜å…ˆæŒ‰å¥å­åˆ†å‰²
        sentences = []
        for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']:
            if delimiter in text:
                sentences = text.split(delimiter)
                # é‡æ–°åŠ ä¸Šåˆ†éš”ç¬¦
                sentences = [s + delimiter for s in sentences[:-1]] + [sentences[-1]] if sentences else []
                break
        
        if not sentences:
            # å¦‚æœæ²¡æœ‰å¥å­åˆ†éš”ç¬¦ï¼ŒæŒ‰é€—å·åˆ†å‰²
            sentences = [s + 'ï¼Œ' for s in text.split('ï¼Œ')[:-1]] + [text.split('ï¼Œ')[-1]] if 'ï¼Œ' in text else [text]
        
        current_chunk = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk + sentence) <= max_chars:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _correct_text_chunk(self, chunk, language):
        """æ ¡æ­£å•ä¸ªæ–‡æœ¬å—"""
        if language == 'zh':
            return self._correct_chinese_chunk(chunk)
        elif language == 'en':
            return self._correct_english_chunk(chunk)
        else:
            return self._correct_multilingual_chunk(chunk, language)
    
    def _correct_chinese_chunk(self, chunk):
        """æ ¡æ­£ä¸­æ–‡æ–‡æœ¬å—"""
        prompt = f"""
è¯·å¯¹ä»¥ä¸‹ä¸­æ–‡è½¬å½•æ–‡æœ¬è¿›è¡Œæ™ºèƒ½æ ¡æ­£å’Œä¼˜åŒ–ï¼š

åŸæ–‡ï¼š
{chunk}

ä»»åŠ¡ï¼š
1. åŒéŸ³å­—çº é”™ï¼šçº æ­£æ˜æ˜¾çš„åŒéŸ³å­—é”™è¯¯ï¼ˆå¦‚ï¼šåœ¨â†’å†ï¼Œçš„â†’å¾—ï¼Œå’Œâ†’åˆç­‰ï¼‰
2. æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–ï¼šæ·»åŠ é€‚å½“çš„é€—å·ã€å¥å·ï¼Œæ”¹å–„å¥å­æµç•…åº¦
3. æ–­å¥ä¼˜åŒ–ï¼šå°†è¿‡é•¿å¥å­åˆç†åˆ†å‰²ï¼Œå°†è¿‡çŸ­ç‰‡æ®µåˆå¹¶
4. è¯­æ³•å®Œå–„ï¼šä¿®æ­£æ˜æ˜¾çš„è¯­æ³•é”™è¯¯ï¼Œä¿æŒè‡ªç„¶è¡¨è¾¾

è¦æ±‚ï¼š
- ä¿æŒåŸæ„å’Œè¯­è¨€é£æ ¼ä¸å˜
- ä¼˜å…ˆä¿®æ­£æ˜æ˜¾é”™è¯¯ï¼Œé¿å…è¿‡åº¦ä¿®æ”¹
- ç¡®ä¿æ¯ä¸ªå¥å­å®Œæ•´ä¸”æ„æ€æ¸…æ™°
- é€‚å½“æ·»åŠ æ ‡ç‚¹ç¬¦å·æé«˜å¯è¯»æ€§
- ç›´æ¥è¿”å›æ ¡æ­£åçš„æ–‡æœ¬ï¼Œæ— éœ€è§£é‡Š

æ ¡æ­£åçš„æ–‡æœ¬ï¼š
"""
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,  # ç¨é«˜æ¸©åº¦å…è®¸åˆ›é€ æ€§æ ¡æ­£
                max_tokens=1000
            )
            
            corrected_text = response.choices[0].message.content.strip()
            
            # è®¡ç®—ä¿®æ­£æ•°é‡
            corrections = self._count_corrections(chunk, corrected_text)
            
            return corrected_text, corrections
            
        except Exception as e:
            self.log(f"âš ï¸ æ–‡æœ¬å—æ ¡æ­£å¤±è´¥: {str(e)}")
            return chunk, 0
    
    def _correct_english_chunk(self, chunk):
        """æ ¡æ­£è‹±æ–‡æ–‡æœ¬å—"""
        prompt = f"""
Please correct and optimize the following English transcript:

Original:
{chunk}

Tasks:
1. Fix obvious transcription errors and typos
2. Add appropriate punctuation (commas, periods) for readability
3. Break long sentences and merge short fragments appropriately
4. Correct grammar while maintaining natural speech patterns

Requirements:
- Preserve original meaning and speaking style
- Focus on clear errors, avoid over-editing
- Ensure each sentence is complete and clear
- Add punctuation to improve readability
- Return only the corrected text without explanations

Corrected text:
"""
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=1000
            )
            
            corrected_text = response.choices[0].message.content.strip()
            corrections = self._count_corrections(chunk, corrected_text)
            
            return corrected_text, corrections
            
        except Exception as e:
            self.log(f"âš ï¸ English chunk correction failed: {str(e)}")
            return chunk, 0
    
    def _correct_multilingual_chunk(self, chunk, language):
        """æ ¡æ­£å¤šè¯­è¨€æ–‡æœ¬å—"""
        lang_name = LanguageConfig.get_language_name(language)
        
        prompt = f"""
Please correct and optimize the following {lang_name} transcript:

Original:
{chunk}

Tasks:
1. Fix transcription errors and improve accuracy
2. Add appropriate punctuation for better readability
3. Optimize sentence structure and flow
4. Maintain natural speaking style

Requirements:
- Preserve original meaning and tone
- Focus on clear improvements
- Ensure sentences are complete and clear
- Return only the corrected text

Corrected text:
"""
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=1000
            )
            
            corrected_text = response.choices[0].message.content.strip()
            corrections = self._count_corrections(chunk, corrected_text)
            
            return corrected_text, corrections
            
        except Exception as e:
            self.log(f"âš ï¸ {lang_name} chunk correction failed: {str(e)}")
            return chunk, 0
    
    def _count_corrections(self, original, corrected):
        """ä¼°ç®—æ ¡æ­£æ•°é‡"""
        if original == corrected:
            return 0
        
        # ç®€å•ä¼°ç®—ï¼šåŸºäºå­—ç¬¦å·®å¼‚å’Œæ ‡ç‚¹å˜åŒ–
        char_diff = abs(len(corrected) - len(original))
        punct_orig = sum(1 for c in original if c in 'ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š')
        punct_corr = sum(1 for c in corrected if c in 'ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š')
        punct_diff = abs(punct_corr - punct_orig)
        
        return max(1, char_diff // 5 + punct_diff)
    
    def _calculate_text_quality_score(self, text):
        """è®¡ç®—æ–‡æœ¬è´¨é‡è¯„åˆ†"""
        score = 5.0  # åŸºç¡€åˆ†
        
        # å¥å­å®Œæ•´æ€§ï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰
        sentences = len([c for c in text if c in 'ã€‚ï¼ï¼Ÿ.!?'])
        total_chars = len(text)
        if total_chars > 0:
            sentence_density = sentences / (total_chars / 100)  # æ¯100å­—ç¬¦çš„å¥å­æ•°
            if 0.5 <= sentence_density <= 2.0:  # ç†æƒ³èŒƒå›´
                score += 2.0
            elif 0.2 <= sentence_density <= 3.0:
                score += 1.0
        
        # æ ‡ç‚¹ç¬¦å·ä¸°å¯Œåº¦
        punct_variety = len(set(text) & set('ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š,.!?;:'))
        score += min(2.0, punct_variety * 0.3)
        
        # æ–‡æœ¬æµç•…åº¦ï¼ˆè¿ç»­æ ‡ç‚¹æˆ–é‡å¤å­—ç¬¦æ‰£åˆ†ï¼‰
        if 'ï¼Œï¼Œ' in text or 'ã€‚ã€‚' in text or '  ' in text:
            score -= 0.5
        
        return min(10.0, score)
    
    def translate_transcript(self, video_id, target_language='en', source_language=None):
        """ç¿»è¯‘å­—å¹•åˆ°ç›®æ ‡è¯­è¨€"""
        try:
            self.log(f"ğŸŒ å¼€å§‹ç¿»è¯‘å­—å¹•åˆ° {LanguageConfig.get_language_name(target_language)}...")
            
            # è·å–è¯­è¨€ä¿¡æ¯
            lang_info = self.db.get_language_info(video_id)
            if not source_language:
                source_language = lang_info.get('detected_language', 'zh') if lang_info else 'zh'
            
            # è¯»å–åŸå§‹è½¬å½•æ–‡æœ¬
            video_info = self.db.get_video_info(video_id)
            if not video_info:
                raise Exception("è§†é¢‘ä¿¡æ¯ä¸å­˜åœ¨")
            
            # æŸ¥æ‰¾è½¬å½•æ–‡ä»¶
            youtube_url = video_info['youtube_url']
            yt_video_id = self.extract_video_id(youtube_url)
            transcript_file = f"transcripts/{yt_video_id}.txt"
            
            if not os.path.exists(transcript_file):
                raise Exception("è½¬å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿›è¡Œè½¬å½•")
            
            # è¯»å–è½¬å½•å†…å®¹
            with open(transcript_file, 'r', encoding='utf-8') as f:
                source_text = f.read()
            
            # æ‰§è¡Œç¿»è¯‘
            translated_text = self._translate_text_with_gpt(source_text, source_language, target_language)
            
            # ä¿å­˜ç¿»è¯‘ç»“æœ
            self._save_translation_files(yt_video_id, translated_text, source_text, target_language, source_language)
            
            # æ›´æ–°æ•°æ®åº“
            self.db.update_language_info(video_id, target_language=target_language)
            self.db.update_translation_status(video_id, True)
            
            # æ›´æ–°å¯ç”¨è¯­è¨€åˆ—è¡¨
            available_languages = lang_info.get('available_languages', []) if lang_info else []
            if source_language not in available_languages:
                available_languages.append(source_language)
            if target_language not in available_languages:
                available_languages.append(target_language)
            self.db.update_available_languages(video_id, available_languages)
            
            self.log(f"âœ… ç¿»è¯‘å®Œæˆ: {LanguageConfig.get_language_name(source_language)} â†’ {LanguageConfig.get_language_name(target_language)}")
            
            return translated_text
            
        except Exception as e:
            self.log(f"âŒ ç¿»è¯‘å¤±è´¥: {str(e)}")
            raise Exception(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
    
    def _translate_text_with_gpt(self, text, source_lang, target_lang):
        """ä½¿ç”¨GPTç¿»è¯‘æ–‡æœ¬"""
        source_lang_name = LanguageConfig.get_language_name(source_lang)
        target_lang_name = LanguageConfig.get_language_name(target_lang)
        
        # åˆ†æ®µç¿»è¯‘ï¼Œé¿å…tokené™åˆ¶
        max_chars_per_chunk = 2000
        chunks = self._split_text_for_correction(text, max_chars_per_chunk)
        
        translated_chunks = []
        
        for i, chunk in enumerate(chunks):
            self.log(f"ğŸ“ ç¿»è¯‘ç¬¬ {i+1}/{len(chunks)} æ®µæ–‡æœ¬...")
            
            translated_chunk = self._translate_chunk(chunk, source_lang_name, target_lang_name, source_lang, target_lang)
            translated_chunks.append(translated_chunk)
        
        return ' '.join(translated_chunks)
    
    def _translate_chunk(self, chunk, source_lang_name, target_lang_name, source_lang, target_lang):
        """ç¿»è¯‘å•ä¸ªæ–‡æœ¬å—"""
        if source_lang == target_lang:
            return chunk
        
        # æ„å»ºç¿»è¯‘æç¤º
        prompt = self._build_translation_prompt(chunk, source_lang_name, target_lang_name, source_lang, target_lang)
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # ç¨é«˜æ¸©åº¦ç¡®ä¿æµç•…ç¿»è¯‘
                max_tokens=1200
            )
            
            translated_text = response.choices[0].message.content.strip()
            return translated_text
            
        except Exception as e:
            self.log(f"âš ï¸ ç¿»è¯‘å—å¤±è´¥: {str(e)}")
            return chunk  # è¿”å›åŸæ–‡
    
    def _build_translation_prompt(self, text, source_lang_name, target_lang_name, source_lang, target_lang):
        """æ„å»ºç¿»è¯‘æç¤º"""
        
        if target_lang == 'zh':
            return f"""
è¯·å°†ä»¥ä¸‹{source_lang_name}æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼š

åŸæ–‡ï¼š
{text}

ç¿»è¯‘è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„æ„æ€å’Œè¯­è°ƒ
2. ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¸­æ–‡è¡¨è¾¾
3. å¯¹äºä¸“ä¸šæœ¯è¯­ï¼Œæä¾›å‡†ç¡®çš„ä¸­æ–‡å¯¹åº”
4. ä¿æŒå¥å­ç»“æ„çš„é€»è¾‘æ€§
5. å¦‚æœæ˜¯å£è¯­åŒ–å†…å®¹ï¼Œç¿»è¯‘ä¹Ÿè¦ä¿æŒå£è¯­åŒ–é£æ ¼
6. ç›´æ¥è¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Š

ä¸­æ–‡ç¿»è¯‘ï¼š
"""
        elif target_lang == 'en':
            return f"""
Please translate the following {source_lang_name} text into English:

Original:
{text}

Translation requirements:
1. Preserve the original meaning and tone
2. Use natural and fluent English expressions
3. Provide accurate English equivalents for technical terms
4. Maintain logical sentence structure
5. If it's conversational content, keep the conversational style
6. Return only the translation without explanations

English translation:
"""
        else:
            return f"""
Please translate the following {source_lang_name} text into {target_lang_name}:

Original:
{text}

Translation requirements:
1. Preserve the original meaning and tone
2. Use natural and fluent {target_lang_name} expressions
3. Maintain the logical flow of ideas
4. Keep the original style (formal/informal)
5. Return only the translation without explanations

{target_lang_name} translation:
"""
    
    def _save_translation_files(self, video_id, translated_text, original_text, target_lang, source_lang):
        """ä¿å­˜ç¿»è¯‘æ–‡ä»¶"""
        # ç¡®ä¿translationsç›®å½•å­˜åœ¨
        os.makedirs('transcripts/translations', exist_ok=True)
        
        # ä¿å­˜ç¿»è¯‘åçš„æ–‡æœ¬æ–‡ä»¶
        target_txt_file = f"transcripts/translations/{video_id}_{target_lang}.txt"
        with open(target_txt_file, 'w', encoding='utf-8') as f:
            f.write(translated_text)
        
        # ä¿å­˜åŸæ–‡ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ä¿å­˜è¿‡ï¼‰
        source_txt_file = f"transcripts/translations/{video_id}_{source_lang}.txt"
        if not os.path.exists(source_txt_file):
            with open(source_txt_file, 'w', encoding='utf-8') as f:
                f.write(original_text)
        
        self.log(f"ğŸ’¾ ç¿»è¯‘æ–‡ä»¶å·²ä¿å­˜: {target_txt_file}")
        
        return target_txt_file
    
    def get_available_translations(self, video_id):
        """è·å–è§†é¢‘çš„å¯ç”¨ç¿»è¯‘"""
        try:
            youtube_url = self.db.get_video_info(video_id)['youtube_url']
            yt_video_id = self.extract_video_id(youtube_url)
            
            translations = {}
            
            # æ£€æŸ¥translationsç›®å½•
            translations_dir = 'transcripts/translations'
            if os.path.exists(translations_dir):
                import glob
                pattern = f"{translations_dir}/{yt_video_id}_*.txt"
                files = glob.glob(pattern)
                
                for file in files:
                    filename = os.path.basename(file)
                    # æå–è¯­è¨€ä»£ç : {video_id}_{lang}.txt
                    lang_code = filename.split('_')[-1].replace('.txt', '')
                    if lang_code in LanguageConfig.SUPPORTED_LANGUAGES:
                        translations[lang_code] = {
                            'language': LanguageConfig.get_language_name(lang_code),
                            'file_path': file,
                            'exists': True
                        }
            
            return translations
            
        except Exception as e:
            self.log(f"âŒ è·å–ç¿»è¯‘åˆ—è¡¨å¤±è´¥: {str(e)}")
            return {}

    def _clean_text_for_matching(self, text):
        """æ¸…ç†æ–‡æœ¬ç”¨äºåŒ¹é…"""
        if not text:
            return ""
        
        import re
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼ï¼Œè½¬æ¢ä¸ºå°å†™
        cleaned = re.sub(r'[^\w\s]', '', text.lower())
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    
    def _calculate_text_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0
        
        # ä½¿ç”¨ç®€å•çš„è¯æ±‡é‡å ç®—æ³•
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = words1 & words2
        union = words1 | words2
        
        if not union:
            return 0
        
        return len(intersection) / len(union)
    
    def _find_partial_match(self, quote_clean, segments):
        """å¯»æ‰¾éƒ¨åˆ†åŒ¹é…çš„æ®µè½ - ä½¿ç”¨åŸºäºè¯æ±‡é‡å åº¦çš„æ™ºèƒ½åŒ¹é…"""
        quote_words = quote_clean.split()
        if len(quote_words) < 3:  # å¤ªçŸ­çš„å¼•ç”¨ä¸è¿›è¡Œéƒ¨åˆ†åŒ¹é…
            return self._get_fallback_segment(segments)
        
        best_match = None
        best_score = 0
        
        # ä¸ºæ¯ä¸ªæ®µè½è®¡ç®—åŒ¹é…åˆ†æ•°
        for segment in segments:
            segment_clean = self._clean_text_for_matching(segment.get('text', ''))
            if not segment_clean:
                continue
                
            segment_words = segment_clean.split()
            
            # è®¡ç®—è¯æ±‡é‡å åº¦
            overlap_score = self._calculate_word_overlap(quote_words, segment_words)
            
            # é¢å¤–å¥–åŠ±ï¼šæ£€æŸ¥å¼€å¤´å’Œç»“å°¾çš„åŒ¹é…
            if self._has_partial_overlap(quote_words, segment_words):
                overlap_score += 0.2  # ç»™éƒ¨åˆ†åŒ¹é…é¢å¤–åˆ†æ•°
            
            if overlap_score > best_score:
                best_score = overlap_score
                best_match = segment
        
        # å¦‚æœæœ‰åˆç†çš„åŒ¹é…ï¼Œè¿”å›æœ€ä½³åŒ¹é…
        if best_match and best_score >= 0.15:  # é™ä½é˜ˆå€¼åˆ°15%
            self.log(f"ğŸ“ éƒ¨åˆ†åŒ¹é…: æ‰¾åˆ° {best_score:.2f} åˆ†æ•°åŒ¹é…")
            return best_match
        
        # æ²¡æœ‰å¥½çš„åŒ¹é…ï¼Œä½¿ç”¨æ™ºèƒ½å›é€€ç­–ç•¥
        return self._get_fallback_segment(segments)
    
    def _has_partial_overlap(self, words1, words2):
        """æ£€æŸ¥ä¸¤ä¸ªè¯æ±‡åˆ—è¡¨æ˜¯å¦æœ‰éƒ¨åˆ†é‡å """
        if len(words1) < 3 or len(words2) < 3:
            return False
        
        # æ£€æŸ¥å¼€å¤´3ä¸ªè¯çš„åŒ¹é…
        start_match = len(set(words1[:3]) & set(words2[:3])) >= 2
        
        # æ£€æŸ¥ç»“å°¾3ä¸ªè¯çš„åŒ¹é…  
        end_match = len(set(words1[-3:]) & set(words2[-3:])) >= 2
        
        return start_match or end_match

    def _get_fallback_segment(self, segments):
        """æ™ºèƒ½å›é€€ç­–ç•¥ - é¿å…æ€»æ˜¯ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰‡æ®µï¼ˆstart=0ï¼‰"""
        if not segments:
            return None
            
        # è¿‡æ»¤æ‰å¼€å§‹æ—¶é—´ä¸º0çš„ç‰‡æ®µï¼ˆé™¤éå®ƒä»¬æ˜¯å”¯ä¸€é€‰æ‹©ï¼‰
        non_zero_segments = [seg for seg in segments if seg.get('start', 0) > 0]
        
        if non_zero_segments:
            # è¿”å›ä¸­é—´ä½ç½®çš„ç‰‡æ®µï¼Œè€Œä¸æ˜¯ç¬¬ä¸€ä¸ª
            middle_index = len(non_zero_segments) // 2
            selected_segment = non_zero_segments[middle_index]
            self.log(f"ğŸ”„ æ™ºèƒ½å›é€€: é€‰æ‹©ä¸­é—´ç‰‡æ®µ (start={selected_segment.get('start', 0)})")
            return selected_segment
        
        # å¦‚æœæ‰€æœ‰ç‰‡æ®µéƒ½æ˜¯start=0ï¼Œæˆ–è€…æ²¡æœ‰å…¶ä»–é€‰æ‹©ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
        if segments:
            self.log(f"âš ï¸ æ™ºèƒ½å›é€€: ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰‡æ®µ (start={segments[0].get('start', 0)})")
            return segments[0]
        
        return None

    def _srt_time_to_seconds(self, time_str):
        """å°†SRTæ—¶é—´æ ¼å¼è½¬æ¢ä¸ºç§’æ•°"""
        # æ ¼å¼ï¼š00:01:23,456 -> 83.456
        time_part, milliseconds = time_str.split(',')
        hours, minutes, seconds = map(int, time_part.split(':'))
        total_seconds = hours * 3600 + minutes * 60 + seconds + int(milliseconds) / 1000
        return total_seconds

    def _merge_summaries(self, summaries):
        """åˆå¹¶å¤šä¸ªæ‘˜è¦"""
        if not summaries:
            return "æ— æ³•ç”Ÿæˆæ‘˜è¦"
        
        # ç®€å•åˆå¹¶ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ç”¨AIå†æ¬¡æ€»ç»“
        combined = "ã€‚".join(summaries)
        return combined

    def _merge_key_points(self, all_key_points):
        """åˆå¹¶å¹¶å»é‡å…³é”®è¦ç‚¹"""
        # ç®€å•å»é‡å’Œé™åˆ¶æ•°é‡
        seen_points = set()
        merged_points = []
        
        for point in all_key_points:
            point_key = point.get('point', '')
            if point_key not in seen_points and len(merged_points) < 8:
                seen_points.add(point_key)
                merged_points.append(point)
        
        return merged_points
    
    def generate_report_html(self, video_title, youtube_url, analysis, srt_file):
        """ç”ŸæˆHTMLç®€æŠ¥"""
        try:
            # æå–YouTubeè§†é¢‘ID
            video_id = self.extract_video_id(youtube_url)
            
            # è¯»å–å¹¶è§£æSRTå­—å¹•æ•°æ®
            subtitles_data = []
            if os.path.exists(srt_file):
                with open(srt_file, 'r', encoding='utf-8') as f:
                    srt_content = f.read()
                
                # è§£æSRTæ ¼å¼
                import re
                pattern = r'(\d+)\s*\n(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})\s*\n(.*?)(?=\n\d+\s*\n|\Z)'
                matches = re.findall(pattern, srt_content, re.DOTALL)
                
                for match in matches:
                    subtitle_id, start_time, end_time, text = match
                    # å°†æ—¶é—´è½¬æ¢ä¸ºç§’æ•°
                    start_seconds = self._srt_time_to_seconds(start_time)
                    end_seconds = self._srt_time_to_seconds(end_time)
                    
                    subtitles_data.append({
                        'id': int(subtitle_id),
                        'start': start_seconds,
                        'end': end_seconds,
                        'text': text.strip().replace('\n', ' ')
                    })
            
            # å°†å­—å¹•æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œä¾›JavaScriptä½¿ç”¨
            import json
            subtitles_json = json.dumps(subtitles_data, ensure_ascii=False)
            html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{video_title} - è§†é¢‘ç®€æŠ¥</title>
    <style>
        * {{ box-sizing: border-box; }}
        body {{
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            background-color: #f8f9fa;
        }}

        .container {{
            max-width: 1600px;
            margin: 0 auto;
            padding: 20px;
        }}

        /* é¡¶éƒ¨åŒºåŸŸï¼šè§†é¢‘ + æ‘˜è¦ */
        .top-section {{
            background: #fff;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}

        .header {{
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 15px;
            flex-wrap: wrap;
            gap: 10px;
        }}

        .header h1 {{
            margin: 0;
            flex: 1;
            min-width: 200px;
        }}

        .header-actions {{
            display: flex;
            gap: 10px;
            align-items: center;
        }}

        .source-btn {{
            background: linear-gradient(135deg, #ff6b6b, #ee5a24);
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 600;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: all 0.3s ease;
            box-shadow: 0 2px 8px rgba(238, 90, 36, 0.3);
        }}

        .source-btn:hover {{
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(238, 90, 36, 0.4);
        }}

        .gen-time {{
            color: #666;
            font-size: 0.85em;
        }}

        .video-container {{
            background: #000;
            border-radius: 8px;
            overflow: hidden;
            margin-bottom: 15px;
        }}

        .video-wrapper {{
            position: relative;
            padding-bottom: 56.25%;
            height: 0;
            overflow: hidden;
        }}

        .video-wrapper iframe {{
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }}

        .summary {{
            background: #e3f2fd;
            padding: 15px 20px;
            border-radius: 8px;
            border-left: 4px solid #2196f3;
        }}

        .summary h2 {{
            margin: 0 0 10px 0;
            font-size: 1.1em;
            color: #1565c0;
        }}

        .summary p {{
            margin: 0;
            color: #333;
        }}

        /* ä¸‹æ–¹åˆ†å±åŒºåŸŸ */
        .main-content {{
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            min-height: 60vh;
        }}

        /* å·¦ä¾§ï¼šå…³é”®è¦ç‚¹ */
        .key-points {{
            background: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            max-height: calc(100vh - 400px);
            overflow-y: auto;
        }}

        .key-points h2 {{
            margin: 0 0 15px 0;
            position: sticky;
            top: 0;
            background: #fff;
            padding: 5px 0 10px 0;
            border-bottom: 2px solid #f0f0f0;
        }}

        .key-point {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 8px;
            transition: transform 0.2s ease;
        }}

        .key-point:hover {{
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }}

        .key-point h3 {{
            margin: 0 0 8px 0;
            font-size: 1em;
            color: #333;
        }}

        .key-point > p {{
            margin: 0 0 10px 0;
            color: #555;
            font-size: 0.95em;
        }}

        .timestamp {{
            background: #007bff;
            color: white;
            padding: 6px 12px;
            border-radius: 20px;
            text-decoration: none;
            cursor: pointer;
            font-size: 0.85em;
            font-weight: 500;
            transition: all 0.2s ease;
            display: inline-block;
        }}

        .timestamp:hover {{
            background: #0056b3;
            transform: scale(1.05);
        }}

        .quote {{
            font-style: italic;
            color: #6c757d;
            margin-top: 10px;
            padding: 10px;
            background: #f1f3f4;
            border-left: 4px solid #007bff;
            border-radius: 4px;
            font-size: 0.9em;
        }}

        /* å³ä¾§ï¼šå®Œæ•´å­—å¹• */
        .subtitles-section {{
            background: #fff;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            display: flex;
            flex-direction: column;
            max-height: calc(100vh - 400px);
        }}

        .subtitles-header {{
            padding: 15px 20px;
            background: #28a745;
            color: white;
            border-radius: 8px 8px 0 0;
            font-weight: 600;
            font-size: 1.1em;
            display: flex;
            align-items: center;
            gap: 8px;
        }}

        .subtitles-container {{
            flex: 1;
            overflow-y: auto;
            padding: 0;
        }}

        .subtitle-line {{
            padding: 10px 20px;
            border-bottom: 1px solid #f0f0f0;
            cursor: pointer;
            transition: all 0.2s ease;
            display: flex;
            align-items: flex-start;
            gap: 12px;
        }}

        .subtitle-line:hover {{
            background: #e3f2fd;
        }}

        /* åŒæ—¶é—´æˆ³åˆ†ç»„é«˜äº® */
        .subtitle-line.group-hover {{
            background: #fff3e0;
        }}

        .subtitle-line.active {{
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            font-weight: 500;
        }}

        .subtitle-line.clicked {{
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
        }}

        @keyframes highlight {{
            0% {{ background: #ffecb3; }}
            100% {{ background: #fff3cd; }}
        }}

        .subtitle-time {{
            color: #007bff;
            font-weight: bold;
            font-size: 0.85em;
            min-width: 50px;
            flex-shrink: 0;
        }}

        .subtitle-text {{
            color: #333;
            flex: 1;
        }}

        /* å“åº”å¼è®¾è®¡ */
        @media (max-width: 1024px) {{
            .main-content {{
                grid-template-columns: 1fr;
            }}

            .key-points,
            .subtitles-section {{
                max-height: 400px;
            }}
        }}

        @media (max-width: 768px) {{
            .container {{
                padding: 10px;
            }}

            .header {{
                flex-direction: column;
            }}

            .header-actions {{
                width: 100%;
                justify-content: space-between;
            }}

            .main-content {{
                gap: 15px;
            }}

            .key-point, .summary {{
                padding: 12px;
            }}
        }}

        /* æ»šåŠ¨æ¡ç¾åŒ– */
        .key-points::-webkit-scrollbar,
        .subtitles-container::-webkit-scrollbar {{
            width: 8px;
        }}

        .key-points::-webkit-scrollbar-track,
        .subtitles-container::-webkit-scrollbar-track {{
            background: #f1f1f1;
            border-radius: 4px;
        }}

        .key-points::-webkit-scrollbar-thumb,
        .subtitles-container::-webkit-scrollbar-thumb {{
            background: #c1c1c1;
            border-radius: 4px;
        }}

        .key-points::-webkit-scrollbar-thumb:hover,
        .subtitles-container::-webkit-scrollbar-thumb:hover {{
            background: #a1a1a1;
        }}
    </style>
</head>
<body>
    <div class="container">
        <!-- é¡¶éƒ¨åŒºåŸŸï¼šæ ‡é¢˜ + è§†é¢‘ + æ‘˜è¦ -->
        <div class="top-section">
            <div class="header">
                <h1>{video_title}</h1>
                <div class="header-actions">
                    <a href="{youtube_url}" target="_blank" class="source-btn">
                        â–¶ï¸ åŸè§†é¢‘
                    </a>
                    <span class="gen-time">ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M')}</span>
                </div>
            </div>

            <div class="video-container">
                <div class="video-wrapper">
                    <div id="youtube-player"></div>
                </div>
            </div>

            <div class="summary">
                <h2>ğŸ“‹ å†…å®¹æ‘˜è¦</h2>
                <p>{analysis['summary']}</p>
            </div>
        </div>

        <!-- ä¸‹æ–¹åˆ†å±åŒºåŸŸ -->
        <div class="main-content">
            <!-- å·¦ä¾§ï¼šå…³é”®è¦ç‚¹ -->
            <div class="key-points">
                <h2>ğŸ”‘ å…³é”®è¦ç‚¹</h2>
"""
            
            for i, point in enumerate(analysis['key_points'], 1):
                timestamp_seconds = point.get('timestamp', 0)
                # ç¡®ä¿timestampæ˜¯æ•°å­—ç±»å‹
                try:
                    timestamp_seconds = float(timestamp_seconds) if timestamp_seconds else 0
                except (ValueError, TypeError):
                    timestamp_seconds = 0
                
                timestamp_display = self.seconds_to_display_time(timestamp_seconds)
                
                html_content += f"""
        <div class="key-point">
            <h3>{i}. {point['point']}</h3>
            <p>{point['explanation']}</p>
            <p><span class="timestamp" onclick="seekToTime({int(timestamp_seconds)})">â° {timestamp_display}</span></p>
            {f'<div class="quote">"{point["quote"]}"</div>' if point.get('quote') else ''}
        </div>
"""
            
            html_content += f"""
            </div>

            <!-- å³ä¾§ï¼šå®Œæ•´å­—å¹• -->
            <div class="subtitles-section">
                <div class="subtitles-header">
                    ğŸ“ å®Œæ•´å­—å¹•
                </div>
                <div class="subtitles-container" id="subtitles-container">
                    <div id="subtitles-list">
                        <!-- å­—å¹•å†…å®¹å°†ç”±JavaScriptåŠ¨æ€ç”Ÿæˆ -->
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- YouTube Player API -->
    <script src="https://www.youtube.com/iframe_api"></script>
    <script>
        let player;
        let subtitlesData = {subtitles_json};
        let currentHighlightedIds = [];  // æ”¯æŒå¤šä¸ªåŒæ—¶é«˜äº®
        let progressUpdateTimer = null;
        let timestampGroups = {{}};  // æŒ‰æ—¶é—´æˆ³åˆ†ç»„çš„å­—å¹•

        // YouTube Player APIå›è°ƒ
        function onYouTubeIframeAPIReady() {{
            player = new YT.Player('youtube-player', {{
                height: '100%',
                width: '100%',
                videoId: '{video_id}',
                playerVars: {{
                    'autoplay': 0,
                    'controls': 1,
                    'rel': 0,
                    'showinfo': 0,
                    'modestbranding': 1
                }},
                events: {{
                    'onReady': onPlayerReady,
                    'onStateChange': onPlayerStateChange
                }}
            }});
        }}

        function onPlayerReady(event) {{
            console.log('YouTube player ready');
            buildTimestampGroups();
            generateSubtitlesList();
        }}

        // æ„å»ºæ—¶é—´æˆ³åˆ†ç»„ï¼ˆæŒ‰ç§’åˆ†ç»„ï¼‰
        function buildTimestampGroups() {{
            subtitlesData.forEach(subtitle => {{
                const timeKey = Math.floor(subtitle.start);
                if (!timestampGroups[timeKey]) {{
                    timestampGroups[timeKey] = [];
                }}
                timestampGroups[timeKey].push(subtitle.id);
            }});
        }}

        // è·å–åŒä¸€æ—¶é—´æˆ³çš„æ‰€æœ‰å­—å¹•ID
        function getSameTimestampIds(subtitleId) {{
            const subtitle = subtitlesData.find(s => s.id === subtitleId);
            if (!subtitle) return [subtitleId];
            const timeKey = Math.floor(subtitle.start);
            return timestampGroups[timeKey] || [subtitleId];
        }}

        function onPlayerStateChange(event) {{
            if (event.data === YT.PlayerState.PLAYING) {{
                startProgressMonitoring();
            }} else {{
                stopProgressMonitoring();
            }}
        }}

        function startProgressMonitoring() {{
            if (progressUpdateTimer) {{
                clearInterval(progressUpdateTimer);
            }}

            progressUpdateTimer = setInterval(() => {{
                if (player && player.getCurrentTime) {{
                    const currentTime = player.getCurrentTime();
                    updateSubtitleHighlight(currentTime);
                }}
            }}, 250);  // æ›´å¿«çš„æ›´æ–°é¢‘ç‡
        }}

        function stopProgressMonitoring() {{
            if (progressUpdateTimer) {{
                clearInterval(progressUpdateTimer);
                progressUpdateTimer = null;
            }}
        }}

        // æ›´æ–°å­—å¹•é«˜äº® - æ”¯æŒåŒæ—¶é—´æˆ³å¤šå­—å¹•é«˜äº®
        function updateSubtitleHighlight(currentTime) {{
            const currentSubtitles = findCurrentSubtitles(currentTime);
            const newIds = currentSubtitles.map(s => s.id);

            // æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if (JSON.stringify(newIds) === JSON.stringify(currentHighlightedIds)) {{
                return;
            }}

            // ç§»é™¤æ—§é«˜äº®
            currentHighlightedIds.forEach(id => {{
                const el = document.querySelector(`[data-subtitle-id="${{id}}"]`);
                if (el) el.classList.remove('active');
            }});

            // æ·»åŠ æ–°é«˜äº®
            newIds.forEach(id => {{
                const el = document.querySelector(`[data-subtitle-id="${{id}}"]`);
                if (el) {{
                    el.classList.add('active');
                }}
            }});

            // è‡ªåŠ¨æ»šåŠ¨åˆ°ç¬¬ä¸€ä¸ªé«˜äº®å­—å¹•
            if (newIds.length > 0 && newIds[0] !== currentHighlightedIds[0]) {{
                const firstEl = document.querySelector(`[data-subtitle-id="${{newIds[0]}}"]`);
                if (firstEl && !firstEl.isScrolling) {{
                    firstEl.isScrolling = true;
                    firstEl.scrollIntoView({{
                        behavior: 'smooth',
                        block: 'center'
                    }});
                    setTimeout(() => {{
                        firstEl.isScrolling = false;
                    }}, 1000);
                }}
            }}

            currentHighlightedIds = newIds;
        }}

        // æŸ¥æ‰¾å½“å‰æ—¶é—´ç‚¹çš„æ‰€æœ‰å­—å¹•
        function findCurrentSubtitles(currentTime) {{
            return subtitlesData.filter(s =>
                currentTime >= s.start && currentTime <= s.end
            );
        }}

        // è·³è½¬åˆ°æŒ‡å®šæ—¶é—´ - ä¿æŒæ’­æ”¾/æš‚åœçŠ¶æ€
        function seekToTime(seconds, clickedElement = null) {{
            if (player && player.seekTo) {{
                // è®°å½•å½“å‰æ’­æ”¾çŠ¶æ€
                const wasPlaying = player.getPlayerState && player.getPlayerState() === YT.PlayerState.PLAYING;

                player.seekTo(seconds, true);

                // æ ¹æ®ä¹‹å‰çš„çŠ¶æ€å†³å®šæ˜¯å¦æ’­æ”¾
                if (wasPlaying) {{
                    player.playVideo();
                }}
                // å¦‚æœä¹‹å‰æ˜¯æš‚åœçš„ï¼Œå°±ä¿æŒæš‚åœï¼ˆä¸è°ƒç”¨playVideoï¼‰

                // æ·»åŠ ç‚¹å‡»åé¦ˆæ•ˆæœ
                if (clickedElement) {{
                    document.querySelectorAll('.subtitle-line.clicked').forEach(el => {{
                        el.classList.remove('clicked');
                    }});
                    clickedElement.classList.add('clicked');
                    setTimeout(() => {{
                        clickedElement.classList.remove('clicked');
                    }}, 500);
                }}
            }}
        }}

        // ç”Ÿæˆå­—å¹•åˆ—è¡¨
        function generateSubtitlesList() {{
            const subtitlesList = document.getElementById('subtitles-list');

            subtitlesData.forEach(subtitle => {{
                const subtitleDiv = document.createElement('div');
                subtitleDiv.className = 'subtitle-line';
                subtitleDiv.setAttribute('data-subtitle-id', subtitle.id);
                subtitleDiv.setAttribute('data-start', subtitle.start);

                // ç‚¹å‡»è·³è½¬
                subtitleDiv.onclick = (event) => {{
                    event.preventDefault();
                    seekToTime(subtitle.start, subtitleDiv);
                }};

                // hoveræ—¶é«˜äº®åŒæ—¶é—´æˆ³çš„æ‰€æœ‰å­—å¹•
                subtitleDiv.onmouseenter = () => {{
                    const sameIds = getSameTimestampIds(subtitle.id);
                    sameIds.forEach(id => {{
                        const el = document.querySelector(`[data-subtitle-id="${{id}}"]`);
                        if (el) el.classList.add('group-hover');
                    }});
                }};

                subtitleDiv.onmouseleave = () => {{
                    const sameIds = getSameTimestampIds(subtitle.id);
                    sameIds.forEach(id => {{
                        const el = document.querySelector(`[data-subtitle-id="${{id}}"]`);
                        if (el) el.classList.remove('group-hover');
                    }});
                }};

                const timeSpan = document.createElement('span');
                timeSpan.className = 'subtitle-time';
                timeSpan.textContent = formatTime(subtitle.start);

                const textSpan = document.createElement('span');
                textSpan.className = 'subtitle-text';
                textSpan.textContent = subtitle.text;

                subtitleDiv.appendChild(timeSpan);
                subtitleDiv.appendChild(textSpan);
                subtitlesList.appendChild(subtitleDiv);
            }});
        }}

        // æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
        function formatTime(seconds) {{
            const minutes = Math.floor(seconds / 60);
            const secs = Math.floor(seconds % 60);
            return minutes.toString().padStart(2, '0') + ':' + secs.toString().padStart(2, '0');
        }}

        // é”®ç›˜å¿«æ·é”®æ”¯æŒ
        document.addEventListener('keydown', (event) => {{
            if (event.target.tagName === 'INPUT' || event.target.tagName === 'TEXTAREA') {{
                return;
            }}

            switch(event.key) {{
                case ' ': // ç©ºæ ¼é”®æ’­æ”¾/æš‚åœ
                    event.preventDefault();
                    if (player && player.getPlayerState && player.playVideo && player.pauseVideo) {{
                        const state = player.getPlayerState();
                        if (state === YT.PlayerState.PLAYING) {{
                            player.pauseVideo();
                        }} else {{
                            player.playVideo();
                        }}
                    }}
                    break;
                case 'ArrowLeft': // å·¦ç®­å¤´åé€€10ç§’
                    event.preventDefault();
                    if (player && player.seekTo && player.getCurrentTime) {{
                        const currentTime = player.getCurrentTime();
                        player.seekTo(Math.max(0, currentTime - 10), true);
                    }}
                    break;
                case 'ArrowRight': // å³ç®­å¤´å‰è¿›10ç§’
                    event.preventDefault();
                    if (player && player.seekTo && player.getCurrentTime) {{
                        const currentTime = player.getCurrentTime();
                        player.seekTo(currentTime + 10, true);
                    }}
                    break;
            }}
        }});

        // é¡µé¢å¸è½½æ—¶æ¸…ç†å®šæ—¶å™¨
        window.addEventListener('beforeunload', () => {{
            stopProgressMonitoring();
        }});

        // é¡µé¢å¯è§æ€§å˜åŒ–æ—¶çš„ä¼˜åŒ–
        document.addEventListener('visibilitychange', () => {{
            if (document.hidden) {{
                if (progressUpdateTimer) {{
                    clearInterval(progressUpdateTimer);
                    progressUpdateTimer = setInterval(() => {{
                        if (player && player.getCurrentTime) {{
                            updateSubtitleHighlight(player.getCurrentTime());
                        }}
                    }}, 2000);
                }}
            }} else {{
                if (player && player.getPlayerState && player.getPlayerState() === YT.PlayerState.PLAYING) {{
                    startProgressMonitoring();
                }}
            }}
        }});
    </script>
</body>
</html>
"""
            
            # ä¿å­˜HTMLæ–‡ä»¶
            safe_title = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            report_filename = f"{safe_title}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            report_path = f"reports/{report_filename}"
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            return report_filename
            
        except Exception as e:
            raise Exception(f"ç”Ÿæˆç®€æŠ¥å¤±è´¥: {str(e)}")
    
    def seconds_to_display_time(self, seconds):
        """å°†ç§’æ•°è½¬æ¢ä¸ºæ˜¾ç¤ºæ ¼å¼"""
        # ç¡®ä¿è¾“å…¥æ˜¯æ•°å­—ç±»å‹
        try:
            seconds = float(seconds) if seconds else 0
        except (ValueError, TypeError):
            seconds = 0
            
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes:02d}:{secs:02d}"
    
    def process_video(self, video_id, youtube_url):
        """å®Œæ•´çš„è§†é¢‘å¤„ç†æµç¨‹ï¼Œæ”¯æŒæ£€æŸ¥ç‚¹æ¢å¤"""
        self.clear_logs()  # æ¸…é™¤ä¹‹å‰çš„æ—¥å¿—
        
        self.log("="*60)
        self.log("ğŸ¬ å¼€å§‹è§†é¢‘å¤„ç†æµç¨‹")
        self.log(f"ğŸ“¹ è§†é¢‘ID: {video_id}")
        self.log(f"ğŸ”— YouTube URL: {youtube_url}")
        self.log("="*60)
        
        try:
            # æ£€æŸ¥å½“å‰çŠ¶æ€å’Œä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹
            next_checkpoint = self.get_next_checkpoint(video_id)
            if next_checkpoint is None:
                self.log("ğŸ‰ æ‰€æœ‰æ£€æŸ¥ç‚¹å·²å®Œæˆï¼Œæ— éœ€å¤„ç†")
                return
            
            self.log(f"ğŸ“ ä»æ£€æŸ¥ç‚¹å¼€å§‹: {next_checkpoint}")
            self.log("ğŸ“ æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºprocessing...")
            self.db.update_video_status(video_id, 'processing')
            self.log("âœ… æ•°æ®åº“çŠ¶æ€æ›´æ–°å®Œæˆ")
            
            # æ ¹æ®æ£€æŸ¥ç‚¹æ¢å¤å¤„ç†
            audio_file = None
            video_title = None
            transcript = None
            srt_file = None
            segments = None
            
            if next_checkpoint == Checkpoint.DOWNLOAD:
                # 1. ä¸‹è½½éŸ³é¢‘
                self.log("1ï¸âƒ£ æ£€æŸ¥ç‚¹: ä¸‹è½½YouTubeéŸ³é¢‘")
                audio_file, video_title = self.download_audio(youtube_url, video_id)
                self.log(f"âœ… éŸ³é¢‘ä¸‹è½½å®Œæˆ: {audio_file}")
                
                # æ›´æ–°ä¸‹è½½æ£€æŸ¥ç‚¹
                self.db.update_checkpoint(video_id, Checkpoint.DOWNLOAD, CheckpointStatus.COMPLETED, audio_file)
                next_checkpoint = Checkpoint.TRANSCRIBE
            
            if next_checkpoint == Checkpoint.TRANSCRIBE:
                # è·å–éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚æœæ²¡æœ‰ä¸‹è½½ï¼‰
                if not audio_file:
                    checkpoint_status = self.db.get_checkpoint_status(video_id)
                    audio_file = checkpoint_status['audio_file_path']
                    if not audio_file or not os.path.exists(audio_file):
                        raise Exception("éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°ä¸‹è½½")
                
                # 2. æ¨¡å‹æ£€æŸ¥å’Œæ™ºèƒ½é‡åˆ†æ
                self.log("2ï¸âƒ£ æ£€æŸ¥ç‚¹: æ£€æŸ¥Whisperæ¨¡å‹å’Œé‡åˆ†æéœ€æ±‚")
                current_model = self.get_current_optimal_model()
                should_reanalyze, previous_model = self.should_reanalyze_with_better_model(video_id, current_model)
                
                if should_reanalyze:
                    self.log(f"ğŸš€ å°†ä½¿ç”¨æ›´å¥½çš„æ¨¡å‹é‡æ–°åˆ†æ")
                    self.log(f"ğŸ“Š è´¨é‡æå‡é¢„æœŸ: è½¬å½•å‡†ç¡®åº¦ +10-15%")
                    force_retranscribe = True
                else:
                    self.log(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {current_model}")
                    force_retranscribe = False
                
                # 3. è¯­éŸ³è½¬å½•
                self.log("3ï¸âƒ£ æ£€æŸ¥ç‚¹: ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è½¬å½•")
                transcript, srt_file, segments = self.transcribe_audio(audio_file, video_id, force_retranscribe)
                self.log(f"âœ… è¯­éŸ³è½¬å½•å®Œæˆï¼Œå…±{len(segments)}ä¸ªç‰‡æ®µ")
                
                # æ›´æ–°ä½¿ç”¨çš„æ¨¡å‹è®°å½•å’Œè½¬å½•æ£€æŸ¥ç‚¹
                # è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
                actual_model = getattr(self, 'current_model_name', current_model)
                self.db.update_whisper_model(video_id, actual_model)
                self.db.update_checkpoint(video_id, Checkpoint.TRANSCRIBE, CheckpointStatus.COMPLETED, srt_file)
                next_checkpoint = Checkpoint.REPORT
            
            if next_checkpoint == Checkpoint.REPORT:
                # è·å–è½¬å½•æ–‡ä»¶ï¼ˆå¦‚æœæ²¡æœ‰è½¬å½•ï¼‰
                if not transcript or not srt_file:
                    checkpoint_status = self.db.get_checkpoint_status(video_id)
                    srt_file = checkpoint_status['transcript_file_path']
                    if not srt_file or not os.path.exists(srt_file):
                        raise Exception("è½¬å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°è½¬å½•")
                    
                    # è¯»å–è½¬å½•æ–‡ä»¶
                    txt_file = srt_file.replace('.srt', '.txt')
                    if os.path.exists(txt_file):
                        with open(txt_file, 'r', encoding='utf-8') as f:
                            transcript = f.read()
                    else:
                        raise Exception("è½¬å½•æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨")
                    
                    # é‡æ–°è§£æsegmentsï¼ˆç®€åŒ–ç‰ˆï¼‰
                    segments = []
                
                # è·å–è§†é¢‘æ ‡é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if not video_title:
                    try:
                        # å°è¯•ä»æ•°æ®åº“è·å–
                        with self.db.get_connection() as conn:
                            cursor = conn.cursor()
                            cursor.execute('SELECT video_title FROM videos WHERE id=?', (video_id,))
                            result = cursor.fetchone()
                            if result and result[0]:
                                video_title = result[0]
                            else:
                                # ä»YouTube URLé‡æ–°è·å–
                                video_title = self.extract_video_title(youtube_url)
                    except:
                        video_title = "æœªçŸ¥æ ‡é¢˜"
                
                # 4. AIåˆ†æ
                self.log("4ï¸âƒ£ æ£€æŸ¥ç‚¹: ä½¿ç”¨GPT-4è¿›è¡Œå†…å®¹åˆ†æ")
                analysis = self.analyze_content(transcript, segments)
                self.log(f"âœ… å†…å®¹åˆ†æå®Œæˆï¼Œæå–{len(analysis.get('key_points', []))}ä¸ªå…³é”®è¦ç‚¹")
                
                # 5. ç”Ÿæˆç®€æŠ¥
                self.log("5ï¸âƒ£ æ£€æŸ¥ç‚¹: ç”ŸæˆHTMLç®€æŠ¥")
                report_filename = self.generate_report_html(video_title, youtube_url, analysis, srt_file)
                self.log(f"âœ… HTMLç®€æŠ¥ç”Ÿæˆå®Œæˆ: {report_filename}")
                
                # æ›´æ–°ç®€æŠ¥æ£€æŸ¥ç‚¹å’Œæ•°æ®åº“
                self.db.update_checkpoint(video_id, Checkpoint.REPORT, CheckpointStatus.COMPLETED)
                self.db.update_report_filename(video_id, report_filename)
            
            # æœ€ç»ˆçŠ¶æ€æ›´æ–°
            self.log("ğŸ“ æ›´æ–°æœ€ç»ˆçŠ¶æ€...")
            self.db.update_video_status(video_id, 'completed')
            
            self.log("="*60)
            self.log("ğŸ‰ è§†é¢‘å¤„ç†æµç¨‹å…¨éƒ¨å®Œæˆ!")
            if 'report_filename' in locals():
                self.log(f"ğŸ“‹ ç®€æŠ¥æ–‡ä»¶: {report_filename}")
            self.log("="*60)
            
        except Exception as e:
            import traceback
            error_msg = str(e)
            detailed_traceback = traceback.format_exc()
            
            print("="*80)
            print("âŒ VIDEO_PROCESSOR: process_videoå¼‚å¸¸!")
            print(f"   ğŸš¨ é”™è¯¯ä¿¡æ¯: {error_msg}")
            print(f"   ğŸ“ è¯¦ç»†å †æ ˆ:")
            print(detailed_traceback)
            print("="*80)
            
            print(f"ğŸ“Š æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºfailed...")
            self.db.update_video_status(video_id, 'failed', error_msg)
            print(f"âœ… çŠ¶æ€æ›´æ–°å®Œæˆ")
            
            raise Exception(error_msg)
    
    def extract_video_title(self, youtube_url):
        """ä»YouTube URLæå–è§†é¢‘æ ‡é¢˜"""
        try:
            ydl_opts = {
                'quiet': True,
                'no_warnings': True,
            }
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(youtube_url, download=False)
                return info.get('title', 'æœªçŸ¥æ ‡é¢˜')
        except:
            return 'æœªçŸ¥æ ‡é¢˜'